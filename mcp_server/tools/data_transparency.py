# -*- coding: utf-8 -*-
"""
æ•°æ®é€æ˜åº¦æŠ¥å‘Šç³»ç»Ÿ v0.7.6

åŸºäºç¬¬ä¸€æ€§åŸç†è®¾è®¡ï¼š
1. ç”¨æˆ·/AIå¿…é¡»çŸ¥é“æ•°æ®çŠ¶æ€
2. é‡‡æ ·è¿‡ç¨‹å¿…é¡»å¯è¿½è¸ª
3. ç½®ä¿¡åº¦å¿…é¡»æœ‰æ•°ç†ä¾æ®
4. ä¸è¶³ä¹‹å¤„å¿…é¡»æ˜ç¡®å‘ŠçŸ¥

æ ¸å¿ƒè¾“å‡ºï¼š
- data_status: æ•°æ®å……è¶³æ€§è¯„ä¼°
- sampling_trace: é‡‡æ ·è¿‡ç¨‹è¿½è¸ª
- statistical_confidence: ç»Ÿè®¡ç½®ä¿¡åº¦
- recommendations: æ”¹è¿›å»ºè®®
"""

import sys
import os
import math
from typing import Dict, Any, Optional, List
from datetime import datetime

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
netease_path = os.path.join(project_root, 'netease_cloud_music')
if netease_path not in sys.path:
    sys.path.insert(0, netease_path)


# ===== ç»Ÿè®¡å­¦å¸¸é‡ =====

# Cochranå…¬å¼å‚æ•°
Z_95 = 1.96  # 95%ç½®ä¿¡æ°´å¹³
Z_99 = 2.576  # 99%ç½®ä¿¡æ°´å¹³

# é‡‡æ ·é˜ˆå€¼
N_MIN_BASIC = 100      # æœ€ä½å¯æ¥å—
N_MIN_STANDARD = 300   # æ ‡å‡†è¦æ±‚
N_IDEAL = 500          # ç†æƒ³æ ·æœ¬é‡
N_RARE_PATTERN = 299   # ç¨€æœ‰æ¨¡å¼æ£€æµ‹ï¼ˆ1%å‡ºç°ç‡ï¼Œ95%æ•è·ï¼‰


def calculate_margin_of_error(n: int, p: float = 0.5, z: float = Z_95) -> float:
    """
    è®¡ç®—æ¯”ä¾‹ä¼°è®¡çš„è¯¯å·®è¾¹ç•Œ

    Args:
        n: æ ·æœ¬é‡
        p: ä¼°è®¡æ¯”ä¾‹ï¼ˆé»˜è®¤0.5ï¼Œæœ€ä¿å®ˆï¼‰
        z: ç½®ä¿¡æ°´å¹³å¯¹åº”çš„Zå€¼

    Returns:
        è¯¯å·®è¾¹ç•Œï¼ˆå¦‚0.05è¡¨ç¤ºÂ±5%ï¼‰
    """
    if n <= 0:
        return 1.0
    return z * math.sqrt(p * (1 - p) / n)


def calculate_required_sample_size(
    margin_of_error: float = 0.05,
    confidence_level: float = 0.95,
    p: float = 0.5
) -> int:
    """
    è®¡ç®—æ‰€éœ€æ ·æœ¬é‡ï¼ˆCochranå…¬å¼ï¼‰

    Args:
        margin_of_error: æœŸæœ›è¯¯å·®è¾¹ç•Œ
        confidence_level: ç½®ä¿¡æ°´å¹³
        p: ä¼°è®¡æ¯”ä¾‹

    Returns:
        æ‰€éœ€æ ·æœ¬é‡
    """
    z = Z_95 if confidence_level < 0.99 else Z_99
    n = (z ** 2 * p * (1 - p)) / (margin_of_error ** 2)
    return int(math.ceil(n))


def assess_sample_adequacy(n: int, api_total: int = None) -> Dict[str, Any]:
    """
    è¯„ä¼°æ ·æœ¬å……è¶³æ€§

    åŸºäºç¬¬ä¸€æ€§åŸç†ï¼š
    - ç»Ÿè®¡å­¦ï¼šè¯¯å·®è¾¹ç•Œã€ç½®ä¿¡åº¦
    - è¦†ç›–ç‡ï¼šæ ·æœ¬å æ€»ä½“æ¯”ä¾‹
    - ç¨€æœ‰æ¨¡å¼ï¼šèƒ½å¦æ£€æµ‹åˆ°1%å‡ºç°ç‡çš„æ¨¡å¼

    Args:
        n: å½“å‰æ ·æœ¬é‡
        api_total: APIæŠ¥å‘Šçš„æ€»è¯„è®ºæ•°

    Returns:
        å……è¶³æ€§è¯„ä¼°æŠ¥å‘Š
    """
    # è®¡ç®—è¯¯å·®è¾¹ç•Œ
    margin_of_error = calculate_margin_of_error(n)

    # è¯„ä¼°ç­‰çº§
    if n >= N_IDEAL:
        level = "excellent"
        level_zh = "ä¼˜ç§€"
    elif n >= N_MIN_STANDARD:
        level = "good"
        level_zh = "è‰¯å¥½"
    elif n >= N_MIN_BASIC:
        level = "acceptable"
        level_zh = "å¯æ¥å—"
    elif n >= 50:
        level = "limited"
        level_zh = "æœ‰é™"
    else:
        level = "insufficient"
        level_zh = "ä¸è¶³"

    # è¦†ç›–ç‡
    coverage = None
    if api_total and api_total > 0:
        coverage = n / api_total

    # ç¨€æœ‰æ¨¡å¼æ£€æµ‹èƒ½åŠ›
    rare_pattern_detectable = n >= N_RARE_PATTERN

    # å„åˆ†æç±»å‹çš„å¯é æ€§
    reliability = {
        "proportion_estimation": {
            "reliable": n >= 100,
            "margin_of_error": f"Â±{margin_of_error*100:.1f}%",
            "note": "å¦‚æƒ…æ„Ÿå æ¯”ã€ä¸»é¢˜åˆ†å¸ƒ"
        },
        "mean_estimation": {
            "reliable": n >= 50,
            "note": "å¦‚æƒ…æ„Ÿå‡åˆ†"
        },
        "rare_pattern_detection": {
            "reliable": rare_pattern_detectable,
            "detectable_rate": f"â‰¥{math.ceil(3/n*100)}%" if n > 0 else "N/A",
            "note": "å¦‚åè®½è¯„è®ºã€ç‰¹å®šæ¢—"
        },
        "temporal_analysis": {
            "reliable": n >= 30 * 3,  # è‡³å°‘3ä¸ªæ—¶é—´æ®µï¼Œæ¯æ®µ30æ¡
            "note": "éœ€è¦æ¯ä¸ªæ—¶é—´æ®µâ‰¥30æ¡"
        }
    }

    return {
        "sample_size": n,
        "api_total": api_total,
        "coverage": f"{coverage*100:.2f}%" if coverage else "æœªçŸ¥",
        "level": level,
        "level_zh": level_zh,
        "margin_of_error": f"Â±{margin_of_error*100:.1f}%",
        "confidence_level": "95%",
        "rare_pattern_detectable": rare_pattern_detectable,
        "reliability": reliability,
        "thresholds": {
            "current": n,
            "minimum_acceptable": N_MIN_BASIC,
            "standard": N_MIN_STANDARD,
            "ideal": N_IDEAL,
            "gap_to_standard": max(0, N_MIN_STANDARD - n)
        }
    }


def create_transparency_report(
    song_id: str,
    db_count: int,
    api_total: int = None,
    sampling_occurred: bool = False,
    sampling_details: Dict = None
) -> Dict[str, Any]:
    """
    åˆ›å»ºå®Œæ•´çš„é€æ˜åº¦æŠ¥å‘Š

    è¿™æ˜¯v0.7.6çš„æ ¸å¿ƒæ”¹è¿›ï¼šè®©ç”¨æˆ·/AIå®Œå…¨äº†è§£æ•°æ®çŠ¶æ€

    Args:
        song_id: æ­Œæ›²ID
        db_count: æ•°æ®åº“ä¸­çš„è¯„è®ºæ•°
        api_total: APIæŠ¥å‘Šçš„æ€»è¯„è®ºæ•°
        sampling_occurred: æ˜¯å¦è¿›è¡Œäº†é‡‡æ ·
        sampling_details: é‡‡æ ·è¿‡ç¨‹è¯¦æƒ…

    Returns:
        é€æ˜åº¦æŠ¥å‘Š
    """
    adequacy = assess_sample_adequacy(db_count, api_total)

    # æ•°æ®æ¥æºè¿½è¸ª
    data_source = {
        "database_count": db_count,
        "api_total": api_total if api_total else "æœªæŸ¥è¯¢",
        "source_type": "cached" if not sampling_occurred else "fresh_sampled",
        "last_check": datetime.now().strftime("%Y-%m-%d %H:%M")
    }

    # é‡‡æ ·è¿½è¸ª
    sampling_trace = None
    if sampling_occurred and sampling_details:
        sampling_trace = {
            "occurred": True,
            "strategy": sampling_details.get("strategy", "unknown"),
            "target": sampling_details.get("target", "N/A"),
            "actual": sampling_details.get("actual", db_count),
            "stop_reason": sampling_details.get("stop_reason", "unknown"),
            "pages_fetched": sampling_details.get("pages_fetched", "N/A"),
            "stability_achieved": sampling_details.get("stability_achieved", None)
        }
    else:
        sampling_trace = {
            "occurred": False,
            "reason": "ä½¿ç”¨ç¼“å­˜æ•°æ®" if db_count > 0 else "æ— æ•°æ®"
        }

    # ç”Ÿæˆå»ºè®®
    recommendations = []

    if adequacy["level"] == "insufficient":
        recommendations.append({
            "priority": "critical",
            "action": "éœ€è¦é‡‡æ ·æ›´å¤šæ•°æ®",
            "detail": f"å½“å‰{db_count}æ¡ï¼Œè‡³å°‘éœ€è¦{N_MIN_BASIC}æ¡",
            "command": f"get_comments_metadata_tool(song_id='{song_id}', include_api_count=True)"
        })
    elif adequacy["level"] == "limited":
        recommendations.append({
            "priority": "high",
            "action": "å»ºè®®å¢åŠ é‡‡æ ·",
            "detail": f"å½“å‰{db_count}æ¡ï¼Œå»ºè®®è¾¾åˆ°{N_MIN_STANDARD}æ¡",
            "gap": N_MIN_STANDARD - db_count
        })

    if not adequacy["rare_pattern_detectable"]:
        recommendations.append({
            "priority": "medium",
            "action": "ç¨€æœ‰æ¨¡å¼æ£€æµ‹å—é™",
            "detail": f"éœ€è¦{N_RARE_PATTERN}æ¡æ‰èƒ½å¯é æ£€æµ‹1%å‡ºç°ç‡çš„æ¨¡å¼ï¼ˆå¦‚åè®½ï¼‰"
        })

    if api_total is None:
        recommendations.append({
            "priority": "info",
            "action": "å»ºè®®æŸ¥è¯¢APIæ€»æ•°",
            "detail": "å¯è®¡ç®—è¦†ç›–ç‡ï¼Œåˆ¤æ–­æ•°æ®ä»£è¡¨æ€§"
        })

    # AIæ¢ç´¢å»ºè®®
    ai_exploration_hints = []

    if adequacy["level"] in ["insufficient", "limited"]:
        ai_exploration_hints.append(
            "âš ï¸ æ•°æ®é‡æœ‰é™ï¼Œåˆ†æç»“è®ºéœ€è°¨æ…è§£è¯»"
        )

    if not adequacy["rare_pattern_detectable"]:
        ai_exploration_hints.append(
            "âš ï¸ å¯èƒ½é—æ¼ä½é¢‘æ¨¡å¼ï¼ˆå¦‚ç‰¹å®šæ¢—ã€åè®½è¯„è®ºï¼‰"
        )

    if adequacy["coverage"] != "æœªçŸ¥":
        coverage_val = float(adequacy["coverage"].replace("%", ""))
        if coverage_val < 1:
            ai_exploration_hints.append(
                f"â„¹ï¸ è¦†ç›–ç‡ä»…{adequacy['coverage']}ï¼Œé«˜èµçƒ­è¯„å¯èƒ½æœªå…¨éƒ¨æ•è·"
            )

    return {
        "transparency_version": "0.7.6",
        "song_id": song_id,
        "data_source": data_source,
        "sample_adequacy": adequacy,
        "sampling_trace": sampling_trace,
        "recommendations": recommendations,
        "ai_exploration_hints": ai_exploration_hints,
        "statistical_notes": {
            "methodology": "åŸºäºCochranå…¬å¼å’Œä¸­å¿ƒæé™å®šç†",
            "assumptions": [
                "å‡è®¾è¯„è®ºåˆ†å¸ƒè¿‘ä¼¼æ­£æ€ï¼ˆå¯¹äºå¤§æ ·æœ¬æˆç«‹ï¼‰",
                "å‡è®¾é‡‡æ ·æ˜¯éšæœºçš„ï¼ˆå®é™…å¯èƒ½æœ‰åå·®ï¼‰",
                "çƒ­è¯„é‡‡æ ·å¯èƒ½è¿‡åº¦ä»£è¡¨æç«¯è§‚ç‚¹"
            ]
        }
    }


def format_transparency_for_ai(report: Dict[str, Any]) -> str:
    """
    æ ¼å¼åŒ–é€æ˜åº¦æŠ¥å‘Šï¼Œä¾›AIé˜…è¯»

    Args:
        report: é€æ˜åº¦æŠ¥å‘Š

    Returns:
        æ ¼å¼åŒ–çš„æ–‡æœ¬
    """
    lines = []
    lines.append("=" * 50)
    lines.append("æ•°æ®é€æ˜åº¦æŠ¥å‘Š")
    lines.append("=" * 50)

    adequacy = report.get("sample_adequacy", {})
    source = report.get("data_source", {})

    lines.append(f"\nğŸ“Š æ ·æœ¬é‡: {adequacy.get('sample_size', 0)}æ¡")
    lines.append(f"ğŸ“ˆ APIæ€»æ•°: {source.get('api_total', 'æœªçŸ¥')}")
    lines.append(f"ğŸ“‰ è¦†ç›–ç‡: {adequacy.get('coverage', 'æœªçŸ¥')}")
    lines.append(f"ğŸ¯ å……è¶³æ€§: {adequacy.get('level_zh', 'æœªçŸ¥')} ({adequacy.get('level', '')})")
    lines.append(f"ğŸ“ è¯¯å·®è¾¹ç•Œ: {adequacy.get('margin_of_error', 'N/A')}")

    lines.append(f"\nç¨€æœ‰æ¨¡å¼æ£€æµ‹: {'å¯é ' if adequacy.get('rare_pattern_detectable') else 'ä¸å¯é '}")

    # å»ºè®®
    recommendations = report.get("recommendations", [])
    if recommendations:
        lines.append("\nâš ï¸ å»ºè®®:")
        for rec in recommendations:
            lines.append(f"  [{rec.get('priority', '')}] {rec.get('action', '')}")
            lines.append(f"      {rec.get('detail', '')}")

    # AIæç¤º
    hints = report.get("ai_exploration_hints", [])
    if hints:
        lines.append("\nğŸ’¡ AIæ¢ç´¢æç¤º:")
        for hint in hints:
            lines.append(f"  {hint}")

    return "\n".join(lines)


# ===== å¯¼å‡º =====

__all__ = [
    "calculate_margin_of_error",
    "calculate_required_sample_size",
    "assess_sample_adequacy",
    "create_transparency_report",
    "format_transparency_for_ai",
    "N_MIN_BASIC",
    "N_MIN_STANDARD",
    "N_IDEAL",
    "N_RARE_PATTERN"
]
