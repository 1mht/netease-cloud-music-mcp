"""
ç¤¾ä¼šå­¦åˆ†æå·¥å…·æ¨¡å—
ä¸“æ³¨äºæ£€æµ‹è¯„è®ºä¸­çš„ç¤¾ä¼šéšå–»ã€é›†ä½“æƒ…ç»ªå’Œè¯è¯­ç­–ç•¥
"""

import sys
import os
import logging
import random
from typing import List, Dict, Any

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
netease_path = os.path.join(project_root, 'netease_cloud_music')
if netease_path not in sys.path:
    sys.path.insert(0, netease_path)

from database import init_db, Comment, Song  # v0.6.6: æ·»åŠ Songæ¨¡å‹
from .workflow_errors import workflow_error  # v0.6.6: ç»Ÿä¸€é”™è¯¯å¤„ç†

logger = logging.getLogger(__name__)

# ===== ç»Ÿè®¡å­¦å¸¸é‡ï¼ˆv0.6.5ï¼‰=====
MAX_ANALYSIS_SIZE = 5000           # v0.6.5: å†…å­˜å®‰å…¨ä¸Šé™
SAMPLE_SIZE_RANDOM = 3000          # éšæœºé‡‡æ ·æ•°é‡
SAMPLE_SIZE_FILTERED = 1000        # è¿‡æ»¤é‡‡æ ·æ•°é‡ï¼ˆtop_liked, recentï¼‰

def get_session():
    """è·å–æ•°æ®åº“session"""
    db_path = os.path.join(project_root, 'data', 'music_data_v2.db')
    return init_db(f'sqlite:///{db_path}')

def detect_social_metaphors(song_id: str, sampling_strategy: str = "auto") -> Dict[str, Any]:
    """
    [ç¤¾ä¼šå­¦è¿›é˜¶] éšå–»ä¸è¯è¯­æ£€æµ‹å™¨ - æ£€æµ‹è¯„è®ºä¸­çš„ç¤¾ä¼šéšå–»å’Œè¯è¯­ç­–ç•¥

    âœ… å¯ç›´æ¥è°ƒç”¨ï¼Œå·¥å…·ä¼šè‡ªåŠ¨å¤„ç†æ•°æ® (v0.7.1)

    ğŸ“‹ ç®€åŒ–è°ƒç”¨æ–¹å¼:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ç›´æ¥è°ƒç”¨: detect_social_metaphors_tool(song_id)             â”‚
    â”‚                                                             â”‚
    â”‚ å·¥å…·å†…éƒ¨è‡ªåŠ¨:                                               â”‚
    â”‚ - æ£€æŸ¥æ­Œæ›²å’Œè¯„è®ºæ˜¯å¦å­˜åœ¨                                     â”‚
    â”‚ - å¦‚æœæ•°æ®ä¸è¶³ä¼šè¿”å›workflow_erroræç¤º                       â”‚
    â”‚ - å¤§æ•°æ®é›†è‡ªåŠ¨é‡‡æ ·ï¼Œé¿å…è¶…æ—¶                                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Š æ•°æ®è¦æ±‚:
    - æœ€ä½: 100æ¡è¯„è®º
    - æ¨è: 300æ¡è¯„è®ºï¼ˆéšå–»æ£€æµ‹æ›´å¯é ï¼‰

    Args:
        song_id: æ­Œæ›²ID
        sampling_strategy: é‡‡æ ·ç­–ç•¥ï¼Œå¹³è¡¡è¦†ç›–ç‡ä¸æ€§èƒ½
            - "auto": æ™ºèƒ½åˆ¤æ–­ (é»˜è®¤)ã€‚å¦‚æœ > 5000 æ¡ï¼Œè‡ªåŠ¨åˆ‡æ¢ä¸º random_sampleã€‚
            - "full": å¼ºåˆ¶å…¨é‡ (æ…ç”¨ï¼Œå¯èƒ½è¶…æ—¶)ã€‚
            - "random_sample": éšæœºæŠ½å– 3000 æ¡ (é€‚åˆå¤§è§„æ¨¡æ¦‚è§ˆ)ã€‚
            - "top_liked": åªçœ‹ç‚¹èµæœ€é«˜çš„ 1000 æ¡ (é€‚åˆçœ‹ä¸»æµå…±è¯†)ã€‚
            - "recent": åªçœ‹æœ€æ–°çš„ 1000 æ¡ (é€‚åˆçœ‹å³æ—¶èˆ†è®º)ã€‚
    """
    # ===== å‚æ•°éªŒè¯ =====
    valid_strategies = ["auto", "full", "random_sample", "top_liked", "recent"]
    if sampling_strategy not in valid_strategies:
        return {
            "status": "error",
            "message": f"æ— æ•ˆçš„é‡‡æ ·ç­–ç•¥: {sampling_strategy}",
            "valid_options": valid_strategies
        }

    session = get_session()
    try:
        # v0.6.6: æ£€æŸ¥æ­Œæ›²æ˜¯å¦å­˜åœ¨
        song = session.query(Song).filter_by(id=song_id).first()
        if not song:
            return workflow_error("song_not_found", "detect_social_metaphors_tool")

        # ä¼˜åŒ–ï¼šå…ˆåªæŸ¥æ•°é‡ï¼Œå†³å®šæ˜¯å¦éœ€è¦å…¨é‡åŠ è½½
        total_count = session.query(Comment).filter_by(song_id=song_id).count()

        if total_count == 0:
            return workflow_error("no_comments", "detect_social_metaphors_tool")

        # é‡‡æ ·é€»è¾‘
        query = session.query(Comment).filter_by(song_id=song_id)
        
        strategy_used = sampling_strategy
        limit = total_count

        if sampling_strategy == "auto":
            if total_count > MAX_ANALYSIS_SIZE:
                strategy_used = "random_sample"
            else:
                strategy_used = "full"

        comments = []
        if strategy_used == "full":
            # v0.6.5: å³ä½¿æ˜¯å…¨é‡ä¹Ÿè¦é™åˆ¶ä¸Šé™
            comments = query.limit(MAX_ANALYSIS_SIZE).all()
        elif strategy_used == "random_sample":
            # v0.6.5: ä½¿ç”¨å¸¸é‡æ›¿ä»£ç¡¬ç¼–ç å€¼
            candidates = query.limit(MAX_ANALYSIS_SIZE * 2).all()
            limit = SAMPLE_SIZE_RANDOM
            comments = random.sample(candidates, min(len(candidates), limit))
        elif strategy_used == "top_liked":
            limit = SAMPLE_SIZE_FILTERED
            comments = query.order_by(Comment.liked_count.desc()).limit(limit).all()
        elif strategy_used == "recent":
            limit = SAMPLE_SIZE_FILTERED
            comments = query.order_by(Comment.timestamp.desc()).limit(limit).all()
        else:
            # é»˜è®¤å…¨é‡ï¼ˆå¸¦ä¸Šé™ï¼‰
            comments = query.limit(MAX_ANALYSIS_SIZE).all()

        total_analyzed = len(comments)
        
        # å®šä¹‰éšå–»æ¨¡å¼ (åŸºäºç¤¾ä¼šå­¦ç ”ç©¶çš„å…³é”®è¯æ˜ å°„)
        patterns = {
            "Nationalism": {
                "keywords": ["ä¸­å›½", "ç¬¬ä¸€", "å›½å®¶", "è‡ªè±ª", "éª„å‚²", "å¼ºå¤§", "å‰å®³", "ç¥–å›½", "ä¸»æƒ"],
                "theory": "å®‰å¾·æ£®: 'æƒ³è±¡çš„å…±åŒä½“' è¯è¯­å®è·µ",
                "count": 0,
                "examples": []
            },
            "Resistance_Irony": {
                "keywords": ["å·¥èµ„", "ç¼“å‘", "ç§©åº", "è®½åˆº", "é˜´é˜³", "åè®½", "æ‡‚çš„éƒ½æ‡‚", "è®¡åˆ’", "ç–‘ä¼¼", "æ³„éœ²"],
                "theory": "æ–¯ç§‘ç‰¹: 'å¼±è€…çš„æ­¦å™¨' / éšç§˜æ–‡æœ¬ (Hidden Transcript)",
                "count": 0,
                "examples": []
            },
            "Identity": {
                "keywords": ["æˆ‘ä»¬", "è¿™ä»£äºº", "é›†ä½“", "æ‰“å¡", "è§è¯", "å†å²", "çˆ·é’å›", "ç ´é˜²", "DNA"],
                "theory": "å¡”è²å°”: ç¤¾ä¼šè®¤åŒç†è®º / ä»ªå¼æ€§å‚ä¸",
                "count": 0,
                "examples": []
            },
            "Hyperreality": {
                "keywords": ["POV", "æ¢—", "å›¢å»º", "ä¹å­", "æŠ½è±¡", "æ´»", "æ•´æ´»", "ç‹‚æ¬¢"],
                "theory": "é²å¾·é‡Œäºš: 'ä»¿çœŸä¸å†…çˆ†' / ç¬¦å·ä¼˜å…ˆäºå†…å®¹",
                "count": 0,
                "examples": []
            }
        }
        
        for c in comments:
            content = c.content
            if not content: continue
            
            for p_name, p_data in patterns.items():
                found_keywords = [k for k in p_data["keywords"] if k in content]
                if found_keywords:
                    p_data["count"] += 1
                    if len(p_data["examples"]) < 3 and len(content) < 100:
                        p_data["examples"].append(content)

        # æ•´ç†ç»“æœ
        findings = []
        for p_name, p_data in patterns.items():
            ratio = p_data["count"] / total_analyzed  # v0.6.6: ä¿®å¤å˜é‡åé”™è¯¯
            findings.append({
                "metaphor_type": p_name,
                "occurrence_ratio": round(ratio, 4),
                "occurrence_percent": f"{ratio:.1%}",
                "sociological_theory": p_data["theory"],
                "evidence_keywords": p_data["keywords"][:5],
                "sample_quotes": p_data["examples"]
            })

        # æŒ‰é¢‘ç‡æ’åº
        findings.sort(key=lambda x: x['occurrence_ratio'], reverse=True)

        return {
            "status": "success",
            "song_id": song_id,
            "total_available": total_count,
            "total_analyzed": total_analyzed,
            "sampling_strategy": strategy_used,
            "metaphor_analysis": findings,
            "summary_for_ai": "è¯·ç»“åˆ occurrence_ratio å’Œ sociological_theory è¿›è¡Œæ·±åº¦è§£è¯»ã€‚é«˜æ¯”ä¾‹çš„ Resistance_Irony é€šå¸¸æš—ç¤ºè¯„è®ºåŒºå­˜åœ¨è§£æ„ä¸»ä¹‰æƒ…ç»ªã€‚"
        }

    except Exception as e:
        logger.error(f"éšå–»æ£€æµ‹å¤±è´¥: {e}")
        return {"status": "error", "message": str(e)}
    finally:
        session.close()
