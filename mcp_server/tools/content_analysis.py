"""
å†…å®¹åˆ†æå·¥å…·æ¨¡å— (NLP)
ä¸“æ³¨äºè¯„è®ºæ–‡æœ¬çš„æ·±åº¦æŒ–æ˜ï¼šå…³é”®è¯æå–ã€è¯é¢˜èšç±»ç­‰
"""

import sys
import os
import logging
import re
import random
from typing import List, Dict, Any

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
netease_path = os.path.join(project_root, 'netease_cloud_music')
if netease_path not in sys.path:
    sys.path.insert(0, netease_path)

from database import init_db, Comment, Song  # v0.6.6: æ·»åŠ Songæ¨¡å‹
from .workflow_errors import workflow_error  # v0.6.6: ç»Ÿä¸€é”™è¯¯å¤„ç†

logger = logging.getLogger(__name__)

# ===== ç»Ÿè®¡å­¦å¸¸é‡ï¼ˆv0.6.4ï¼‰=====
MAX_ANALYSIS_SIZE = 5000           # å†…å­˜å®‰å…¨ï¼šæœ€å¤§åˆ†ææ•°é‡
DEGRADED_MODE_THRESHOLD = 5        # â‰¤5æ¡ï¼šé™çº§æ¨¡å¼
MIN_VIABLE_SIZE = 30               # 30æ¡ï¼šæœ€å°å¯åˆ†æ
RECOMMENDED_SIZE = 100             # 100æ¡ï¼šå»ºè®®çº¿

def get_session():
    """è·å–æ•°æ®åº“session"""
    db_path = os.path.join(project_root, 'data', 'music_data_v2.db')
    return init_db(f'sqlite:///{db_path}')

def classify_comments(song_id: str, sampling_strategy: str = "auto") -> Dict[str, Any]:
    """
    [æ ¸å¿ƒå‡çº§] è¯„è®ºæˆåˆ†åˆ†ç±»å™¨ - å°†è¯„è®ºåˆ†ä¸ºæ•…äº‹/ç©æ¢—/ä¹è¯„/çŸ­è¯„

    âœ… å¯ç›´æ¥è°ƒç”¨ï¼Œå·¥å…·ä¼šè‡ªåŠ¨å¤„ç†æ•°æ®ã€‚

    ğŸ“‹ ç®€åŒ–è°ƒç”¨æ–¹å¼ (v0.7.1):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ç›´æ¥è°ƒç”¨: classify_comments_tool(song_id)                   â”‚
    â”‚                                                             â”‚
    â”‚ å·¥å…·å†…éƒ¨è‡ªåŠ¨:                                               â”‚
    â”‚ - æ£€æŸ¥æ•°æ®åº“è¯„è®ºæ•°é‡                                        â”‚
    â”‚ - å¦‚æœæ•°æ®ä¸è¶³ä¼šè¿”å›workflow_erroræç¤º                       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Š æ•°æ®è¦æ±‚:
    - æœ€ä½: 100æ¡è¯„è®º
    - æ¨è: 200æ¡è¯„è®ºï¼ˆåˆ†ç±»æ›´å‡†ç¡®ï¼‰

    Args:
        song_id: æ­Œæ›²ID
        sampling_strategy: é‡‡æ ·ç­–ç•¥ ("auto", "full", "random_sample")
    """
    session = get_session()
    try:
        # v0.6.6: æ£€æŸ¥æ­Œæ›²æ˜¯å¦å­˜åœ¨
        song = session.query(Song).filter_by(id=song_id).first()
        if not song:
            return workflow_error("song_not_found", "classify_comments_tool")

        # è·å–æ€»æ•°
        total_count = session.query(Comment).filter_by(song_id=song_id).count()

        if total_count == 0:
            return workflow_error("no_comments", "classify_comments_tool")

        # ===== Phase 1: å†…å­˜å®‰å…¨æ£€æŸ¥ï¼ˆv0.6.5ä¼˜åŒ–ï¼‰=====
        # åªåœ¨ç”¨æˆ·å¼ºåˆ¶å…¨é‡æ—¶æ‰è­¦å‘Šï¼Œautoæ¨¡å¼ä¼šè‡ªåŠ¨é‡‡æ ·
        if total_count > MAX_ANALYSIS_SIZE and sampling_strategy == "full":
            return {
                "status": "dataset_too_large",
                "message": f"âš ï¸ æ•°æ®é›†è¿‡å¤§ï¼š{total_count}æ¡è¯„è®º",
                "current_size": total_count,
                "max_allowed": MAX_ANALYSIS_SIZE,
                "recommendation": {
                    "action": "ä½¿ç”¨é‡‡æ ·åˆ†æ",
                    "suggested_call": f"classify_comments(song_id='{song_id}', sampling_strategy='auto')",
                    "why": "autoæ¨¡å¼ä¼šè‡ªåŠ¨é‡‡æ ·ï¼Œé¿å…å†…å­˜æº¢å‡º"
                }
            }

        # é‡‡æ ·é€»è¾‘ï¼ˆæ”¹è¿›ï¼šä¸Šé™5000ï¼‰
        query = session.query(Comment).filter_by(song_id=song_id)

        strategy_used = sampling_strategy
        if sampling_strategy == "auto":
            strategy_used = "random_sample" if total_count > MAX_ANALYSIS_SIZE else "full"

        if strategy_used == "random_sample":
            # æ”¹è¿›ï¼šé™åˆ¶é‡‡æ ·ä¸Šé™ä¸ºMAX_ANALYSIS_SIZE
            candidates = query.limit(MAX_ANALYSIS_SIZE * 2).all()
            sample_size = min(len(candidates), MAX_ANALYSIS_SIZE)
            comments = random.sample(candidates, sample_size)
        else:
            comments = query.limit(MAX_ANALYSIS_SIZE).all()  # å®‰å…¨ä¸Šé™

        # ===== Phase 3: åˆ†å±‚æ ·æœ¬é‡æ£€æŸ¥ =====
        # v0.6.6: æ ·æœ¬è¿‡å°‘åŒæ ·éœ€è¦å¼•å¯¼è·å–æ›´å¤šæ•°æ®
        if len(comments) <= DEGRADED_MODE_THRESHOLD:
            return workflow_error("no_comments", "classify_comments_tool")

        # [æ”¹è¿› 2] è¯­è¨€æ£€æµ‹ (ç®€å•å¯å‘å¼)
        # ç»Ÿè®¡åŒ…å«ä¸­æ–‡å­—ç¬¦çš„è¯„è®ºæ¯”ä¾‹
        def has_chinese(text):
            return any('\u4e00' <= char <= '\u9fff' for char in text)
            
        chinese_count = sum(1 for c in comments if c.content and has_chinese(c.content))
        chinese_ratio = chinese_count / len(comments)
        
        language_warning = None
        if chinese_ratio < 0.5:
            language_warning = f"âš ï¸ æ£€æµ‹åˆ°éä¸­æ–‡è¯„è®ºå ä¸»å¯¼ ({100-chinese_ratio*100:.1f}%)ã€‚SnowNLPæƒ…æ„Ÿåˆ†æå’Œç¤¾ä¼šå­¦éšå–»æ£€æµ‹å¯èƒ½å¤±æ•ˆã€‚"

        categories = {
            "Story": [],
            "Meme": [],
            "Review": [],
            "Short": []
        }
        
        # éŸ³ä¹æœ¯è¯­åº“
        music_terms = {'ç¼–æ›²', 'ä½œè¯', 'ä½œæ›²', 'éŸ³è‰²', 'å‰ä»–', 'è´æ–¯', 'é¼“ç‚¹', 'æ··éŸ³', 'å‰å¥', 'å°¾å¥', 'å’Œå£°', 'å”±åŠŸ', 'å—“éŸ³'}
        # æ•…äº‹ç‰¹å¾è¯
        story_indicators = {'é‚£æ—¶å€™', 'è®°å¾—', 'åæ¥', 'æ›¾ç»', 'æ„Ÿè§‰', 'æƒ³èµ·', 'å› ä¸º', 'è™½ç„¶', 'å¹´', 'å²'}
        
        for c in comments:
            content = c.content
            if not content:
                continue
                
            length = len(content)
            
            # 1. åˆ¤å®š Short (è¿‡çŸ­)
            if length < 6:
                categories["Short"].append(c)
                continue
                
            # 2. åˆ¤å®š Review (ä¹è¯„)
            # å¦‚æœåŒ…å«2ä¸ªä»¥ä¸ŠéŸ³ä¹æœ¯è¯­
            term_count = sum(1 for term in music_terms if term in content)
            if term_count >= 1 or (term_count >=1 and length > 20):
                categories["Review"].append(c)
                continue
                
            # 3. åˆ¤å®š Story (æ•…äº‹)
            # é•¿åº¦å¤Ÿé•¿ï¼Œä¸”åŒ…å«ç¬¬ä¸€äººç§°æˆ–å™äº‹è¯
            story_score = 0
            if length > 40: story_score += 2
            if length > 80: story_score += 2
            if 'æˆ‘' in content: story_score += 1
            if any(i in content for i in story_indicators): story_score += 1
            
            if story_score >= 3:
                categories["Story"].append(c)
                continue
                
            # 4. åˆ¤å®š Meme (ç©æ¢—) - å‰©ä¸‹çš„è¾ƒçŸ­ä½†æœ‰ç‰¹è‰²çš„
            # ç½‘æ˜“äº‘çš„æ¢—é€šå¸¸çŸ­å°ç²¾æ‚ï¼Œæˆ–è€…å¸¦æœ‰ç‰¹æ®Šç¬¦å·
            if length < 30 and ('å“ˆå“ˆå“ˆ' in content or '?' in content or 'doge' in content):
                 categories["Meme"].append(c)
                 continue
            
            # é»˜è®¤å½’ç±»
            if length < 15:
                categories["Short"].append(c)
            else:
                # å‰©ä¸‹çš„å½’ä¸º Meme/Other æ··æ‚ï¼Œè¿™é‡Œæš‚æ—¶æ”¾ Short æˆ– Meme è§†æƒ…å†µ
                # ç®€å•èµ·è§ï¼Œä¸­ç­‰é•¿åº¦éæ•…äº‹éä¹è¯„ï¼Œæš‚å½’ Meme (å¹¿ä¹‰çš„åæ§½)
                categories["Meme"].append(c)

        # æ•´ç†è¿”å›ç»“æœ (åªè¿”å›å‰5æ¡ç²¾é€‰ï¼Œé¿å…Tokençˆ†ç‚¸)
        def format_top(clist):
            # æŒ‰ç‚¹èµæ’åº
            sorted_list = sorted(clist, key=lambda x: x.liked_count or 0, reverse=True)[:5]
            return [{"content": x.content, "liked": x.liked_count} for x in sorted_list]

        return {
            "status": "success",
            "song_id": song_id,
            "total_analyzed": len(comments),
            "distribution": {
                "Story": len(categories["Story"]),
                "Meme": len(categories["Meme"]),
                "Review": len(categories["Review"]),
                "Short": len(categories["Short"])
            },
            "distribution_percent": {
                k: f"{len(v)/len(comments):.1%}" for k,v in categories.items()
            },
            "classification": {
                "counts": {
                    "Story": len(categories["Story"]),
                    "Meme": len(categories["Meme"]),
                    "Review": len(categories["Review"]),
                    "Short": len(categories["Short"])
                },
                "percentages": {
                    k: f"{len(v)/len(comments):.1%}" for k,v in categories.items()
                }
            },
            "highlights": {
                "Story": format_top(categories["Story"]),
                "Review": format_top(categories["Review"]),
                "Meme": format_top(categories["Meme"])
            },
            "language_warning": language_warning,
            "note": "Story=å°ä½œæ–‡/æ•…äº‹, Meme=ç©æ¢—/åæ§½, Review=ä¹è¯„/é‰´èµ"
        }

    except Exception as e:
        logger.error(f"åˆ†ç±»å¤±è´¥: {e}")
        return {"status": "error", "message": str(e)}
    finally:
        session.close()

def extract_keywords(song_id: str, top_k: int = 20, sampling_strategy: str = "auto") -> Dict[str, Any]:
    """
    æå–è¯„è®ºåŒºçš„æ ¸å¿ƒå…³é”®è¯ (TF-IDFç®—æ³•)

    âœ… å¯ç›´æ¥è°ƒç”¨ï¼Œå·¥å…·ä¼šè‡ªåŠ¨å¤„ç†æ•°æ® (v0.7.1)

    ğŸ“‹ ç®€åŒ–è°ƒç”¨æ–¹å¼:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ç›´æ¥è°ƒç”¨: extract_keywords_tool(song_id)                    â”‚
    â”‚                                                             â”‚
    â”‚ å·¥å…·å†…éƒ¨è‡ªåŠ¨:                                               â”‚
    â”‚ - æ£€æŸ¥æ­Œæ›²å’Œè¯„è®ºæ˜¯å¦å­˜åœ¨                                     â”‚
    â”‚ - å¦‚æœæ•°æ®ä¸è¶³ä¼šè¿”å›workflow_erroræç¤º                       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Š æ•°æ®è¦æ±‚: æ¨èè‡³å°‘100æ¡è¯„è®ºä»¥è·å¾—å¯é ç»“æœ

    Args:
        song_id: æ­Œæ›²ID
        top_k: è¿”å›å‰Kä¸ªå…³é”®è¯
        sampling_strategy: é‡‡æ ·ç­–ç•¥ ("auto", "full", "random_sample")
    """
    import jieba.analyse

    # ===== å‚æ•°éªŒè¯ =====
    # éªŒè¯ top_k
    if not isinstance(top_k, int) or top_k <= 0 or top_k > 100:
        return {
            "status": "error",
            "message": f"top_k å¿…é¡»æ˜¯ 1-100 ä¹‹é—´çš„æ•´æ•°ï¼Œå½“å‰å€¼: {top_k}",
            "valid_range": "1-100"
        }

    # éªŒè¯ sampling_strategy
    valid_strategies = ["auto", "full", "random_sample", "top_liked", "recent"]
    if sampling_strategy not in valid_strategies:
        return {
            "status": "error",
            "message": f"æ— æ•ˆçš„é‡‡æ ·ç­–ç•¥: {sampling_strategy}",
            "valid_options": valid_strategies
        }

    session = get_session()
    try:
        # v0.6.6: æ£€æŸ¥æ­Œæ›²æ˜¯å¦å­˜åœ¨
        song = session.query(Song).filter_by(id=song_id).first()
        if not song:
            return workflow_error("song_not_found", "extract_keywords_tool")

        # è·å–æ€»æ•°
        total_count = session.query(Comment).filter_by(song_id=song_id).count()
        if total_count == 0:
            return workflow_error("no_comments", "extract_keywords_tool")

        # é‡‡æ ·é€»è¾‘
        query = session.query(Comment).filter_by(song_id=song_id)
        
        strategy_used = sampling_strategy
        if sampling_strategy == "auto":
            strategy_used = "random_sample" if total_count > 5000 else "full"
            
        if strategy_used == "random_sample":
            candidates = query.limit(10000).all()
            comments = random.sample(candidates, min(len(candidates), 3000))
        else:
            comments = query.all()

        text = " ".join([c.content for c in comments if c.content])

        # 2. æå–å…³é”®è¯
        # allowPOS: ä»…æå–åè¯(n)ã€åŠ¨è¯(v)ã€å½¢å®¹è¯(a)ç­‰ï¼Œè¿‡æ»¤æ‰æ— æ„ä¹‰çš„è™šè¯
        tags = jieba.analyse.extract_tags(text, topK=top_k, withWeight=True, allowPOS=('n', 'nr', 'ns', 'nt', 'nz', 'v', 'vd', 'vn', 'a', 'ad', 'an'))

        keywords = [{"word": tag, "weight": round(weight, 4)} for tag, weight in tags]

        return {
            "status": "success",
            "song_id": song_id,
            "total_comments_analyzed": len(comments),
            "algorithm": "TF-IDF",
            "keywords": keywords,
            "note": "æƒé‡(weight)è¶Šé«˜ï¼Œä»£è¡¨è¯¥è¯åœ¨è¯„è®ºåŒºè¶Šé‡è¦ä¸”ç‹¬ç‰¹"
        }

    except Exception as e:
        logger.error(f"å…³é”®è¯æå–å¤±è´¥: {e}")
        return {
            "status": "error",
            "message": f"å…³é”®è¯æå–å¤±è´¥: {str(e)}"
        }
    finally:
        session.close()
