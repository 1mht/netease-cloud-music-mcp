"""
ä¸»é¢˜å»ºæ¨¡å·¥å…·æ¨¡å— (NLP è¿›é˜¶)
ä½¿ç”¨ LDA (Latent Dirichlet Allocation) ç®—æ³•å°†è¯„è®ºèšç±»ä¸ºæ½œåœ¨è¯é¢˜
"""

import sys
import os
import logging
from typing import List, Dict, Any
import re

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
netease_path = os.path.join(project_root, 'netease_cloud_music')
if netease_path not in sys.path:
    sys.path.insert(0, netease_path)

from database import init_db, Comment, Song  # v0.6.6: æ·»åŠ Songæ¨¡å‹
from .workflow_errors import workflow_error  # v0.6.6: ç»Ÿä¸€é”™è¯¯å¤„ç†

logger = logging.getLogger(__name__)

# ===== ç»Ÿè®¡å­¦å¸¸é‡ï¼ˆv0.6.4+v0.6.5ï¼‰=====
MAX_ANALYSIS_SIZE = 5000           # v0.6.5: å†…å­˜å®‰å…¨ä¸Šé™
MIN_VIABLE_SIZE = 30               # 30æ¡ï¼šæœ€å°å¯åˆ†æï¼ˆLDAéœ€è¦æ›´å¤šæ ·æœ¬ï¼‰
RECOMMENDED_SIZE = 100             # 100æ¡ï¼šå»ºè®®çº¿

def get_session():
    """è·å–æ•°æ®åº“session"""
    db_path = os.path.join(project_root, 'data', 'music_data_v2.db')
    return init_db(f'sqlite:///{db_path}')

def clean_text(text: str) -> str:
    """ç®€å•æ¸…æ´—æ–‡æœ¬ï¼šå»é™¤ç©ºç™½å­—ç¬¦"""
    if not text:
        return ""
    # æ›¿æ¢æ‰éæ ‡å‡†å­—ç¬¦ï¼Œé¿å…ç¼–ç é—®é¢˜
    return text.strip()

def perform_topic_modeling(song_id: str, n_topics: int = 3, n_top_words: int = 8) -> Dict[str, Any]:
    """
    å¯¹è¯„è®ºåŒºè¿›è¡Œ LDA ä¸»é¢˜èšç±»åˆ†æ

    ğŸ“‹ å‰ç½®æ¡ä»¶ï¼ˆv0.6.6ï¼‰:
    âœ“ æ­Œæ›²å¿…é¡»å·²å­˜åœ¨äºæ•°æ®åº“ï¼ˆé€šè¿‡searchâ†’confirmâ†’add_songæµç¨‹æ·»åŠ ï¼‰
    âœ“ æ•°æ®åº“ä¸­å¿…é¡»æœ‰è¯„è®ºæ•°æ®ï¼ˆé€šè¿‡get_comments_by_pages_toolè·å–ï¼‰
    âœ“ æ¨èè‡³å°‘100æ¡è¯„è®ºä»¥è·å¾—å¯é çš„ä¸»é¢˜èšç±»ç»“æœ

    âš ï¸ å¦‚æœå‰ç½®æ¡ä»¶ä¸æ»¡è¶³:
    æœ¬å·¥å…·ä¼šè¿”å›workflow_errorï¼ŒæŒ‡å¼•ä½ å®Œæˆæ­£ç¡®æµç¨‹

    æ­£ç¡®è°ƒç”¨é¡ºåºç¤ºä¾‹:
    1. search_songs_tool â†’ confirm_song_selection_tool â†’ add_song_to_database
    2. get_comments_by_pages_tool (è·å–è¯„è®ºæ•°æ®)
    3. ğŸ‘‰ cluster_comments_tool â† å½“å‰å·¥å…·

    è¿™æ˜¯ä¸€ä¸ªæ— ç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œèƒ½è‡ªåŠ¨å‘ç°è¯„è®ºåŒºéšå«çš„å‡ ä¸ªè®¨è®ºæ–¹å‘ï¼ˆTopicï¼‰ã€‚

    Args:
        song_id: æ­Œæ›²ID
        n_topics: å¸Œæœ›å‘ç°å‡ ä¸ªä¸»é¢˜ï¼ˆé»˜è®¤3ä¸ªï¼‰
        n_top_words: æ¯ä¸ªä¸»é¢˜å±•ç¤ºå‰å‡ ä¸ªå…³é”®è¯

    Returns:
        {
            "song_id": "...",
            "topics": [
                {
                    "topic_id": 0,
                    "top_words": ["é’æ˜¥", "å›å¿†", "å­¦æ ¡", "é—æ†¾"],
                    "weight": 0.45  # è¯¥ä¸»é¢˜åœ¨æ‰€æœ‰è¯„è®ºä¸­çš„å æ¯”
                },
                ...
            ]
        }
    """
    import jieba
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.decomposition import LatentDirichletAllocation

    # ===== å‚æ•°éªŒè¯ =====
    if not isinstance(n_topics, int) or n_topics < 2 or n_topics > 20:
        return {
            "status": "error",
            "message": f"n_topics å¿…é¡»æ˜¯ 2-20 ä¹‹é—´çš„æ•´æ•°ï¼Œå½“å‰å€¼: {n_topics}",
            "valid_range": "2-20",
            "suggestion": "å»ºè®®ä½¿ç”¨ 3-5 ä¸ªä¸»é¢˜ä»¥è·å¾—æœ€ä½³æ•ˆæœ"
        }

    if not isinstance(n_top_words, int) or n_top_words < 3 or n_top_words > 20:
        return {
            "status": "error",
            "message": f"n_top_words å¿…é¡»æ˜¯ 3-20 ä¹‹é—´çš„æ•´æ•°ï¼Œå½“å‰å€¼: {n_top_words}",
            "valid_range": "3-20"
        }

    session = get_session()
    try:
        # v0.6.6: æ£€æŸ¥æ­Œæ›²æ˜¯å¦å­˜åœ¨
        song = session.query(Song).filter_by(id=song_id).first()
        if not song:
            return workflow_error("song_not_found", "cluster_comments_tool")

        # 1. è·å–è¯„è®ºæ•°æ®
        comments = session.query(Comment.content).filter_by(song_id=song_id).all()

        # æ•°æ®é‡æ£€æŸ¥
        if not comments:
            return workflow_error("no_comments", "cluster_comments_tool")
            
        raw_documents = [c.content for c in comments if c.content and len(c.content) > 2]

        # v0.6.5: å¤§æ•°æ®é›†é‡‡æ ·ï¼Œé˜²æ­¢å†…å­˜æº¢å‡ºå’ŒLDAè¶…æ—¶
        original_count = len(raw_documents)
        if original_count > MAX_ANALYSIS_SIZE:
            import random
            raw_documents = random.sample(raw_documents, MAX_ANALYSIS_SIZE)
            logger.info(f"[cluster_comments] æ•°æ®é›†è¿‡å¤§({original_count}æ¡)ï¼Œå·²è‡ªåŠ¨é‡‡æ ·åˆ°{MAX_ANALYSIS_SIZE}æ¡")

        # v0.6.6: æ ·æœ¬é‡ä¸è¶³åŒæ ·éœ€è¦å¼•å¯¼è·å–æ›´å¤šæ•°æ®
        if len(raw_documents) < MIN_VIABLE_SIZE:
            return workflow_error("no_comments", "cluster_comments_tool")

        # 2. æ–‡æœ¬é¢„å¤„ç†ä¸åˆ†è¯
        processed_docs = []
        for doc in raw_documents:
            try:
                # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
                if not isinstance(doc, str):
                    continue
                    
                # ä»…ä¿ç•™åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ï¼Œè¿‡æ»¤æ‰æ— æ„ä¹‰è¯æ±‡
                words = jieba.cut(doc.strip())
                
                # ç®€å•çš„åœç”¨è¯è¿‡æ»¤
                stop_words = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'ä¹Ÿ', 'å°±', 'ä¸', 'éƒ½', 'è¿™', 'é‚£', 'æœ‰', 'å•Š', 'å§', 'å‘¢', 'å—', 'user', 'reply'}
                filtered_words = [w for w in words if len(w) > 1 and w not in stop_words]
                
                if filtered_words:
                    processed_docs.append(" ".join(filtered_words))
            except Exception:
                continue

        if not processed_docs:
             return {"status": "error", "message": "é¢„å¤„ç†åæ²¡æœ‰å‰©ä½™æœ‰æ•ˆæ–‡æœ¬ã€‚"}

        # 3. å‘é‡åŒ– (CountVectorizer)
        # max_df=0.95: å¿½ç•¥å‡ºç°åœ¨95%ä»¥ä¸Šæ–‡æ¡£ä¸­çš„è¯ï¼ˆå¤ªé€šç”¨çš„è¯ï¼‰
        # min_df=2: å¿½ç•¥åªå‡ºç°è¿‡ä¸€æ¬¡çš„è¯ï¼ˆå¤ªç”Ÿåƒ»çš„è¯ï¼‰
        tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000)
        tf = tf_vectorizer.fit_transform(processed_docs)
        
        # 4. è®­ç»ƒ LDA æ¨¡å‹
        # n_jobs=1 é¿å… Windows ä¸‹ joblib ä¸´æ—¶æ–‡ä»¶è·¯å¾„å«ä¸­æ–‡å¯¼è‡´çš„ UnicodeEncodeError
        # v0.6.5: max_iter ä» 10 é™è‡³ 5ï¼ŒåŠ å¿«æ”¶æ•›é€Ÿåº¦
        lda = LatentDirichletAllocation(
            n_components=n_topics,
            max_iter=5,
            learning_method='online',
            random_state=42,
            n_jobs=1
        )
        lda.fit(tf)

        # 5. æå–ç»“æœ
        feature_names = tf_vectorizer.get_feature_names_out()
        topics = []
        
        # ç®€å•çš„æƒé‡å½’ä¸€åŒ–ï¼ˆå¹¶ä¸ä¸¥è°¨ï¼Œä»…ä¾›å‚è€ƒï¼‰
        topic_dist = lda.transform(tf)
        topic_weights = topic_dist.sum(axis=0)
        topic_weights /= topic_weights.sum()

        for topic_idx, topic in enumerate(lda.components_):
            top_features_ind = topic.argsort()[:-n_top_words - 1:-1]
            top_words = [feature_names[i] for i in top_features_ind]
            
            topics.append({
                "topic_id": topic_idx + 1,
                "keywords": top_words,
                "importance": round(topic_weights[topic_idx], 2),
                "interpretation_guide": "è¯·æ ¹æ®å…³é”®è¯æ¨æµ‹è¯¥ä¸»é¢˜çš„å«ä¹‰ï¼ˆå¦‚ï¼šæƒ…æ„Ÿå®£æ³„ã€æ­Œè¯è®¨è®ºã€ç©æ¢—ç­‰ï¼‰"
            })

        # æŒ‰é‡è¦æ€§æ’åº
        topics.sort(key=lambda x: x['importance'], reverse=True)

        result = {
            "status": "success",
            "song_id": song_id,
            "algorithm": "LDA (Latent Dirichlet Allocation)",
            "total_documents": len(processed_docs),
            "topics": topics,
            "note": "Keywordsæ˜¯è¯¥ä¸»é¢˜ä¸‹çš„é«˜é¢‘è¯ï¼ŒImportanceæ˜¯è¯¥ä¸»é¢˜åœ¨è¯„è®ºåŒºçš„å¤§è‡´å æ¯”"
        }

        # v0.6.5: æ·»åŠ é‡‡æ ·ä¿¡æ¯
        if original_count > MAX_ANALYSIS_SIZE:
            result["sampling_info"] = {
                "sampled": True,
                "original_count": original_count,
                "sampled_count": MAX_ANALYSIS_SIZE,
                "reason": "æ•°æ®é›†è¿‡å¤§ï¼Œè‡ªåŠ¨é‡‡æ ·ä»¥é˜²æ­¢è¶…æ—¶"
            }

        return result

    except Exception as e:
        # ä½¿ç”¨ repr(e) é¿å…åœ¨é”™è¯¯å¤„ç†æ—¶å†æ¬¡è§¦å‘ç¼–ç é”™è¯¯
        error_msg = repr(e)
        return {
            "status": "error",
            "message": f"ä¸»é¢˜åˆ†æå¤±è´¥: {error_msg}",
            "suggestion": "å¯èƒ½æ˜¯æ•°æ®é‡ä¸è¶³æˆ–scikit-learnç¯å¢ƒé—®é¢˜"
        }
    finally:
        session.close()
