"""
v0.8.5 åˆ†å±‚åˆ†æå·¥å…· - æ¸è¿›å¼æ•°æ®åŠ è½½

è®¾è®¡åŸåˆ™ï¼š
- æ¯ä¸ª Layer æ˜¯ç‹¬ç«‹çš„å·¥å…·è°ƒç”¨
- AI åœ¨æ¯å±‚ä¹‹é—´åšå†³ç­–
- çœ tokenã€å»å™ªéŸ³

Layer æ¶æ„ï¼š
- Layer 0: get_analysis_overview - æ•°æ®è¾¹ç•Œï¼ˆAIç¬¬ä¸€çœ¼ï¼‰
- Layer 1: get_analysis_signals - å…­ç»´åº¦ä¿¡å·ï¼ˆAIç¬¬äºŒçœ¼ï¼‰
- Layer 2: get_analysis_samples - éªŒè¯æ ·æœ¬ï¼ˆAIç¬¬ä¸‰çœ¼ï¼‰
- Layer 3: get_raw_comments_v2 - åŸå§‹è¯„è®ºï¼ˆæŒ‰éœ€ï¼‰

v0.8.5 æ–°å¢ï¼š
- æ¯ä¸ª Layer è¿”å› deeper_options å­—æ®µ
- ç”¨æˆ·å¯ä»¥å¼ºåˆ¶ AI æ·±å…¥åˆ†æç‰¹å®šæ–¹å‘
- AI åˆæ¬¡å¯è‡ªè¡Œå†³å®šæ·±åº¦ï¼Œä½†ç”¨æˆ·æœ‰æœ€ç»ˆæ§åˆ¶æƒ
"""

import sys
import os
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
from collections import defaultdict

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
netease_path = os.path.join(project_root, 'netease_cloud_music')
if netease_path not in sys.path:
    sys.path.insert(0, netease_path)

from database import init_db, Song, Comment
from mcp_server.tools.workflow_errors import workflow_error

logger = logging.getLogger(__name__)

# å¸¸é‡
MAX_ANALYSIS_SIZE = 5000


def get_session():
    """è·å–æ•°æ®åº“session"""
    db_path = os.path.join(project_root, 'data', 'music_data_v2.db')
    return init_db(f'sqlite:///{db_path}')


# ============================================================
# Layer 0: æ•°æ®æ¦‚è§ˆ
# ============================================================

def get_analysis_overview(song_id: str) -> Dict[str, Any]:
    """
    Layer 0: æ•°æ®æ¦‚è§ˆ - AI ç¬¬ä¸€çœ¼çœ‹è¿™é‡Œ

    v0.8.6: åªå±•ç¤ºæ•°æ®è¾¹ç•Œï¼Œä¸åšé‡‡æ ·å†³ç­–
    é‡‡æ ·å†³ç­–åœ¨ Layer 1 ä¹‹åï¼Œæ ¹æ®å„ç»´åº¦çš„ data_sufficiency è¯„ä¼°

    è¿”å›æ•°æ®è¾¹ç•Œä¿¡æ¯ï¼Œå¸®åŠ© AI åˆ¤æ–­ï¼š
    - æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿï¼Ÿ
    - è¦†ç›–èŒƒå›´æ˜¯å¦åˆç†ï¼Ÿ

    Args:
        song_id: æ­Œæ›²ID

    Returns:
        {
            "status": "success",
            "layer": 0,
            "song_info": {...},
            "data_boundary": {
                "db_count": 1234,
                "api_total": 50000,
                "coverage": "2.47%",
                "coverage_ratio": 0.0247,
                "year_span": "2015-01-01 ~ 2024-12-25",
                "years_covered": 10,
                "year_distribution": {...}
            },
            "quality_assessment": {...},
            "ai_guidance": {...},
            "sampling_note": "é‡‡æ ·å†³ç­–åœ¨ Layer 1 ä¹‹å..."
        }
    """
    session = get_session()

    try:
        # 1. è·å–æ­Œæ›²
        song = session.query(Song).filter_by(id=song_id).first()
        if not song:
            return workflow_error("song_not_found", "get_analysis_overview")

        # 2. ç»Ÿè®¡æ•°æ®åº“è¯„è®º
        db_count = session.query(Comment).filter_by(song_id=song_id).count()
        if db_count == 0:
            return workflow_error("no_comments", "get_analysis_overview")

        comments = session.query(Comment).filter_by(song_id=song_id).limit(MAX_ANALYSIS_SIZE).all()

        # 3. è·å– API æ€»é‡
        api_total = 0
        try:
            from mcp_server.tools.pagination_sampling import get_real_comments_count_from_api
            api_result = get_real_comments_count_from_api(song_id)
            api_total = api_result.get("total_comments", 0) if api_result else 0
        except Exception as e:
            logger.warning(f"è·å–APIæ€»é‡å¤±è´¥: {e}")

        # 4. è®¡ç®—æ—¶é—´è·¨åº¦å’Œå¹´ä»½åˆ†å¸ƒ
        year_distribution = defaultdict(int)
        timestamps = []

        for c in comments:
            ts = getattr(c, 'timestamp', 0) or 0
            if ts > 0:
                timestamps.append(ts)
                year = datetime.fromtimestamp(ts / 1000).year
                year_distribution[year] += 1

        year_distribution = dict(sorted(year_distribution.items()))

        if timestamps:
            min_ts, max_ts = min(timestamps), max(timestamps)
            earliest = datetime.fromtimestamp(min_ts / 1000).strftime("%Y-%m-%d")
            latest = datetime.fromtimestamp(max_ts / 1000).strftime("%Y-%m-%d")
            year_span = f"{earliest} ~ {latest}"
        else:
            year_span = "unknown"

        # 5. è¦†ç›–ç‡è®¡ç®—
        coverage = f"{db_count/api_total*100:.2f}%" if api_total > 0 else "unknown"
        coverage_ratio = db_count / api_total if api_total > 0 else 0

        # 6. æ•°æ®è´¨é‡è¯„ä¼°
        MIN_REQUIRED_FOR_ANALYSIS = 100  # æœ€ä½åˆ†æè¦æ±‚
        RECOMMENDED_FOR_ANALYSIS = 500   # æ¨èåˆ†æé‡

        if db_count >= RECOMMENDED_FOR_ANALYSIS:
            quality_level = "good"
            quality_note = "æ ·æœ¬é‡å……è¶³ï¼Œç»Ÿè®¡ç»“æœå¯ä¿¡"
        elif db_count >= 200:
            quality_level = "acceptable"
            quality_note = "æ ·æœ¬é‡å¯æ¥å—ï¼Œç»“æœå¯å‚è€ƒ"
        elif db_count >= MIN_REQUIRED_FOR_ANALYSIS:
            quality_level = "limited"
            quality_note = "æ ·æœ¬é‡æœ‰é™ï¼Œç»“æœç½®ä¿¡åº¦è¾ƒä½"
        else:
            quality_level = "insufficient"
            quality_note = f"æ ·æœ¬é‡ä¸è¶³ï¼ˆéœ€â‰¥{MIN_REQUIRED_FOR_ANALYSIS}æ¡ï¼‰ï¼Œå¿…é¡»å…ˆé‡‡æ ·"

        # 7. v0.8.7: å¼ºåˆ¶é‡‡æ ·æ£€æŸ¥ - æ•°æ®ä¸è¶³æ—¶é˜»æ–­æµç¨‹
        must_sample_first = db_count < MIN_REQUIRED_FOR_ANALYSIS

        if must_sample_first:
            # ç›´æ¥è¿”å›å¼ºåˆ¶é‡‡æ ·æç¤ºï¼Œé˜»æ–­åç»­æµç¨‹
            return {
                "status": "must_sample_first",
                "layer": 0,
                "layer_name": "data_overview",

                "song_info": {
                    "id": song_id,
                    "name": song.name,
                    "artist": song.artists[0].name if song.artists else "Unknown"
                },

                "data_boundary": {
                    "db_count": db_count,
                    "api_total": api_total,
                    "min_required": MIN_REQUIRED_FOR_ANALYSIS
                },

                "blocking_reason": f"â›” æ•°æ®é‡ä¸è¶³ï¼šå½“å‰ä»… {db_count} æ¡è¯„è®ºï¼Œæœ€ä½éœ€è¦ {MIN_REQUIRED_FOR_ANALYSIS} æ¡æ‰èƒ½è¿›è¡Œå¯é åˆ†æ",

                "required_action": {
                    "action": "sample_comments_tool",
                    "params": {"song_id": song_id, "level": "standard"},
                    "instruction": f"âš ï¸ å¿…é¡»å…ˆè°ƒç”¨ sample_comments_tool(song_id='{song_id}', level='standard') è¿›è¡Œé‡‡æ ·ï¼",
                    "reason": "é‡‡æ ·åå†è°ƒç”¨ get_analysis_overview_tool ç»§ç»­åˆ†æ"
                },

                "ai_instruction": "ğŸš« ç¦æ­¢ç»§ç»­è°ƒç”¨ Layer 1/2/3ï¼å¿…é¡»å…ˆå®Œæˆé‡‡æ ·ï¼"
            }

        # 8. æ„å»º AI å¼•å¯¼ï¼ˆæ•°æ®å……è¶³æ—¶ï¼‰
        ai_guidance = {
            "next_action": "è°ƒç”¨ get_analysis_signals_tool(song_id) æŸ¥çœ‹å…­ç»´åº¦ä¿¡å·",
            "when_to_skip": None
        }

        if coverage_ratio < 0.001:  # < 0.1%
            ai_guidance["data_warning"] = "è¦†ç›–ç‡æä½(<0.1%)ï¼Œåˆ†æç»“æœå¯èƒ½æœ‰åå·®ï¼Œè¯·è°¨æ…è§£è¯»"
        elif quality_level == "limited":
            ai_guidance["data_warning"] = "æ ·æœ¬é‡æœ‰é™ï¼Œå¯è€ƒè™‘è¡¥å……é‡‡æ ·ä»¥æé«˜ç½®ä¿¡åº¦"

        # 9. v0.8.4: AI è¾“å‡ºè¦æ±‚ - å¼ºåˆ¶ç™½ç›’åŒ–
        # è§£é‡Š db_count çš„æ¥æº
        db_count_explanation = f"æ•°æ®åº“ä¸­å­˜å‚¨äº† {db_count} æ¡è¯„è®º"
        if db_count >= MAX_ANALYSIS_SIZE:
            db_count_explanation += f"ï¼ˆå—åˆ†æä¸Šé™ {MAX_ANALYSIS_SIZE} é™åˆ¶ï¼‰"

        ai_output_requirements = {
            "must_report": [
                f"æ•°æ®æ¥æºï¼š{db_count_explanation}",
                f"API æ˜¾ç¤ºè¯¥æ­Œæ›²å…±æœ‰ {api_total} æ¡è¯„è®ºï¼Œå½“å‰è¦†ç›–ç‡ {coverage}",
                f"æ—¶é—´èŒƒå›´ï¼š{year_span}",
                f"æ•°æ®è´¨é‡è¯„ä¼°ï¼š{quality_level} - {quality_note}"
            ],
            "format": "æŠ¥å‘Šå¼€å¤´å¿…é¡»è¯´æ˜æ•°æ®è¾¹ç•Œï¼Œè®©ç”¨æˆ·çŸ¥é“ç»“è®ºçš„å¯ä¿¡åº¦",
            "warning": "è¦†ç›–ç‡<1%æ—¶ï¼Œåˆ†æç»“è®ºå¯èƒ½æœ‰æŠ½æ ·åå·®" if coverage_ratio < 0.01 else None,
            # v0.8.5: å†³ç­–é€æ˜åŒ–
            "decision_transparency": {
                "must_explain": "AI å¿…é¡»åœ¨è¾“å‡ºä¸­è¯´æ˜ï¼šä¸ºä»€ä¹ˆå†³å®šç»§ç»­æŸ¥çœ‹ Layer 1ï¼Ÿ",
                "example_continue": "æ•°æ®è´¨é‡ä¸º goodï¼Œè¦†ç›–ç‡ 2%ï¼Œå†³å®šç»§ç»­æŸ¥çœ‹å…­ç»´åº¦ä¿¡å·",
                "example_stop": "æ•°æ®é‡ä¸è¶³ï¼ˆä»… 50 æ¡ï¼‰ï¼Œå»ºè®®å…ˆè¡¥å……é‡‡æ ·ï¼Œæš‚ä¸æ·±å…¥åˆ†æ"
            }
        }

        # 10. v0.8.6: Layer 0 ä¸åšé‡‡æ ·å†³ç­–ï¼Œåªå±•ç¤ºæ•°æ®
        # é‡‡æ ·å†³ç­–åœ¨ Layer 1 ä¹‹åï¼Œæ ¹æ®å„ç»´åº¦æ•°æ®éœ€æ±‚

        return {
            "status": "success",
            "layer": 0,
            "layer_name": "data_overview",

            "song_info": {
                "id": song_id,
                "name": song.name,
                "artist": song.artists[0].name if song.artists else "Unknown",
                "album": song.album.name if song.album else ""
            },

            "data_boundary": {
                "db_count": db_count,
                "api_total": api_total,
                "coverage": coverage,
                "coverage_ratio": coverage_ratio,  # æ•°å€¼å½¢å¼ï¼Œæ–¹ä¾¿åˆ¤æ–­
                "year_span": year_span,
                "years_covered": len(year_distribution),
                "year_distribution": year_distribution
            },

            "quality_assessment": {
                "level": quality_level,
                "note": quality_note
            },

            "ai_guidance": ai_guidance,

            # v0.8.4: å¼ºåˆ¶ AI æŠ¥å‘Šæ•°æ®æ¥æº
            "ai_output_requirements": ai_output_requirements,

            # v0.8.6: é‡‡æ ·æç¤º
            "sampling_note": "é‡‡æ ·å†³ç­–åœ¨ Layer 1 ä¹‹åï¼Œæ ¹æ®å„ç»´åº¦çš„ data_sufficiency è¯„ä¼°ç»“æœå†³å®š"
        }

    except Exception as e:
        logger.error(f"Layer 0 åˆ†æå¤±è´¥: {e}", exc_info=True)
        return {
            "status": "error",
            "error_type": "layer0_failed",
            "message": str(e),
            "song_id": song_id
        }

    finally:
        session.close()


# ============================================================
# Layer 1: å…­ç»´åº¦ä¿¡å·
# ============================================================

def get_analysis_signals(song_id: str) -> Dict[str, Any]:
    """
    Layer 1: å…­ç»´åº¦ä¿¡å· - AI ç¬¬äºŒçœ¼çœ‹è¿™é‡Œ

    è¿”å›å…­ä¸ªç»´åº¦çš„é‡åŒ–æŒ‡æ ‡å’Œå¼‚å¸¸ä¿¡å·ï¼Œå¸®åŠ© AI åˆ¤æ–­ï¼š
    - å“ªäº›ç»´åº¦æœ‰å¼‚å¸¸éœ€è¦å…³æ³¨ï¼Ÿ
    - å“ªäº›ä¿¡å·éœ€è¦é€šè¿‡æ ·æœ¬éªŒè¯ï¼Ÿ

    Args:
        song_id: æ­Œæ›²ID

    Returns:
        {
            "status": "success",
            "layer": 1,
            "dimensions": {
                "sentiment": {"metrics": {...}, "signals": [...], "level": "good"},
                "content": {...},
                "temporal": {...},
                "structural": {...},
                "social": {...},
                "linguistic": {...}
            },
            "cross_dimension_signals": [...],
            "signals_summary": {
                "total": 5,
                "needs_verification": ["åè®½ä¿¡å·", "æ—¶é—´å¼‚å¸¸"]
            },
            "ai_guidance": {...}
        }
    """
    session = get_session()

    try:
        # 1. è·å–æ­Œæ›²
        song = session.query(Song).filter_by(id=song_id).first()
        if not song:
            return workflow_error("song_not_found", "get_analysis_signals")

        # 2. è·å–è¯„è®º
        comments = session.query(Comment).filter_by(song_id=song_id).limit(MAX_ANALYSIS_SIZE).all()
        if not comments:
            return workflow_error("no_comments", "get_analysis_signals")

        # 2.5 v0.8.7: å¼ºåˆ¶æ£€æŸ¥ - æ•°æ®é‡ä¸è¶³æ—¶é˜»æ–­
        MIN_REQUIRED_FOR_ANALYSIS = 100
        comment_count = len(comments)

        if comment_count < MIN_REQUIRED_FOR_ANALYSIS:
            return {
                "status": "must_sample_first",
                "layer": 1,
                "layer_name": "dimension_signals",

                "blocking_reason": f"â›” æ•°æ®é‡ä¸è¶³ï¼šå½“å‰ä»… {comment_count} æ¡è¯„è®ºï¼Œæœ€ä½éœ€è¦ {MIN_REQUIRED_FOR_ANALYSIS} æ¡",

                "required_action": {
                    "action": "sample_comments_tool",
                    "params": {"song_id": song_id, "level": "standard"},
                    "instruction": f"âš ï¸ å¿…é¡»å…ˆè°ƒç”¨ sample_comments_tool(song_id='{song_id}', level='standard') è¿›è¡Œé‡‡æ ·ï¼"
                },

                "correct_flow": [
                    "1. sample_comments_tool(song_id, level='standard') - å…ˆé‡‡æ ·",
                    "2. get_analysis_overview_tool(song_id) - æŸ¥çœ‹æ•°æ®è¾¹ç•Œ",
                    "3. get_analysis_signals_tool(song_id) - å†æŸ¥çœ‹ä¿¡å·"
                ],

                "ai_instruction": "ğŸš« ç¦æ­¢ç»§ç»­ï¼å¿…é¡»å…ˆå®Œæˆé‡‡æ ·ï¼"
            }

        # 3. åˆ†ææ‰€æœ‰ç»´åº¦
        from mcp_server.tools.dimension_analyzers_v2 import analyze_all_dimensions_v2
        dimensions_result = analyze_all_dimensions_v2(comments)

        # 4. æå–è·¨ç»´åº¦ä¿¡å·
        from mcp_server.tools.cross_dimension import detect_cross_signals
        cross_signals = detect_cross_signals(dimensions_result, comments)

        # 5. æå–å„ç»´åº¦æ ¸å¿ƒæŒ‡æ ‡å’Œä¿¡å·ï¼ˆç®€åŒ–ç‰ˆï¼Œä¸å«æ ·æœ¬ï¼‰
        dimensions_summary = {}
        all_signals = []

        # v0.8.6: æ”¶é›†å„ç»´åº¦æ•°æ®å……è¶³æ€§
        insufficient_dimensions = []  # æ•°æ®ä¸è¶³çš„ç»´åº¦
        sampling_recommendations = []  # é‡‡æ ·å»ºè®®

        for dim_name, dim_data in dimensions_result.items():
            if dim_name == "anchor_contrast_samples":
                continue  # æ ·æœ¬åœ¨ Layer 2 è¿”å›

            qf = dim_data.get("quantified_facts", {})
            signals = dim_data.get("signals", [])

            # v0.8.6: æå–æ•°æ®å……è¶³æ€§è¯„ä¼°
            data_suff = dim_data.get("data_sufficiency", {})
            suff_level = data_suff.get("level", "unknown")

            dimensions_summary[dim_name] = {
                "sample_size": qf.get("sample_size", 0),
                "data_sufficiency": data_suff,  # v0.8.6: åŒ…å«å®Œæ•´è¯„ä¼°
                "metrics": qf.get("metrics", {}),
                "signals": signals
            }

            # v0.8.6: æ”¶é›†æ•°æ®ä¸è¶³çš„ç»´åº¦
            if suff_level in ["insufficient", "limited"]:
                need_more = data_suff.get("need_more", 0) or data_suff.get("need_more_years", 0) or data_suff.get("need_more_hot", 0)
                insufficient_dimensions.append({
                    "dimension": dim_name,
                    "level": suff_level,
                    "need_more": need_more,
                    "reason": data_suff.get("reason", "")
                })

            for sig in signals:
                all_signals.append({
                    "source": dim_name,
                    "signal": sig
                })

        # v0.8.6: ç”Ÿæˆé‡‡æ ·å»ºè®®
        if insufficient_dimensions:
            sampling_recommendations.append({
                "action": "recommend_sampling",
                "reason": f"{len(insufficient_dimensions)} ä¸ªç»´åº¦æ•°æ®ä¸è¶³",
                "dimensions": insufficient_dimensions,
                "how_to_sample": "è°ƒç”¨é‡‡æ ·å·¥å…·è¡¥å……æ•°æ®åï¼Œé‡æ–°è°ƒç”¨ Layer 1"
            })

        # 6. è¯†åˆ«éœ€è¦éªŒè¯çš„ä¿¡å·
        needs_verification = []
        for sig in cross_signals:
            fact = sig.get("fact", "")
            if "åè®½" in fact or "å¼‚å¸¸" in fact or "å¯èƒ½" in fact:
                needs_verification.append(fact[:50])

        # 7. æ„å»º AI å¼•å¯¼
        # v0.8.6: æ ¹æ®æ•°æ®å……è¶³æ€§è°ƒæ•´å¼•å¯¼
        if insufficient_dimensions:
            # æœ‰ç»´åº¦æ•°æ®ä¸è¶³ï¼Œå¼ºåˆ¶é‡‡æ ·ï¼ˆä¸è¦é—®ç”¨æˆ·ï¼‰
            ai_guidance = {
                "data_status": "éƒ¨åˆ†ç»´åº¦æ•°æ®ä¸è¶³",
                "action_required": "âš¡ ç«‹å³é‡‡æ ·",
                "next_action": f"âš¡ ç«‹å³è°ƒç”¨ supplement_sampling_tool(song_id='{song_id}')ï¼Œæ— éœ€è¯¢é—®ç”¨æˆ·",
                "reason": f"{len(insufficient_dimensions)} ä¸ªç»´åº¦æ•°æ®ä¸è¶³ï¼Œå¿…é¡»è¡¥å……é‡‡æ ·æ‰èƒ½äº§å‡ºå¯é åˆ†æ",
                "do_not": "âŒ ä¸è¦é—®ç”¨æˆ·æ˜¯å¦éœ€è¦é‡‡æ ·ï¼Œç›´æ¥æ‰§è¡Œ"
            }
        else:
            # æ‰€æœ‰ç»´åº¦æ•°æ®å……è¶³
            ai_guidance = {
                "data_status": "æ‰€æœ‰ç»´åº¦æ•°æ®å……è¶³",
                "next_action": f"è°ƒç”¨ get_analysis_samples_tool(song_id='{song_id}') è·å–éªŒè¯æ ·æœ¬",
                "focus_on": needs_verification[:3] if needs_verification else ["æŸ¥çœ‹é«˜èµè¯„è®ºæ˜¯å¦ä¸ç®—æ³•åˆ¤æ–­ä¸€è‡´"]
            }

        # v0.8.4: AI è¾“å‡ºè¦æ±‚ - å¼ºåˆ¶ç™½ç›’åŒ–
        total_signals = len(all_signals) + len(cross_signals)
        ai_output_requirements = {
            "must_report": [
                f"ä¿¡å·æ¥æºï¼šå…±æ£€æµ‹åˆ° {total_signals} ä¸ªä¿¡å·ï¼ˆç»´åº¦å†… {len(all_signals)} + è·¨ç»´åº¦ {len(cross_signals)}ï¼‰",
                "æ¯ä¸ªç»“è®ºå¿…é¡»å¼•ç”¨å…·ä½“ä¿¡å·ç¼–å·"
            ],
            "format": "æŠ¥å‘Šä¸­æåˆ°ä»»ä½•æ¨¡å¼/ç‰¹å¾æ—¶ï¼Œå¿…é¡»è¯´æ˜æ˜¯åŸºäºå“ªä¸ªä¿¡å·ï¼ˆå¦‚ï¼šæ ¹æ® signal_Xï¼‰",
            "avoid": "é¿å…æ²¡æœ‰æ•°æ®æ”¯æ’‘çš„ä¸»è§‚åˆ¤æ–­ï¼ˆå¦‚'æ˜¾ç„¶æ˜¯'ã€'è‚¯å®šæ˜¯'ï¼‰",
            # v0.8.5: å†³ç­–é€æ˜åŒ–
            "decision_transparency": {
                "must_explain": "AI å¿…é¡»åœ¨è¾“å‡ºä¸­è¯´æ˜ï¼šä¸ºä»€ä¹ˆå†³å®šç»§ç»­æŸ¥çœ‹ Layer 2ï¼ˆéªŒè¯æ ·æœ¬ï¼‰ï¼Ÿæˆ–ä¸ºä»€ä¹ˆåœæ­¢ï¼Ÿ",
                "example_continue": f"æ£€æµ‹åˆ° {len(needs_verification)} ä¸ªéœ€è¦éªŒè¯çš„ä¿¡å·ï¼Œå†³å®šæŸ¥çœ‹æ ·æœ¬éªŒè¯",
                "example_stop": "æ‰€æœ‰ä¿¡å·ç½®ä¿¡åº¦é«˜ï¼Œæ— éœ€æ ·æœ¬éªŒè¯ï¼Œç›´æ¥è¾“å‡ºæŠ¥å‘Š"
            }
        }

        # v0.8.5: ç”¨æˆ·å¼ºåˆ¶æ·±å…¥é€‰é¡¹
        # è¯†åˆ«æœ‰ä¿¡å·çš„ç»´åº¦
        dims_with_signals = [dim for dim, data in dimensions_summary.items() if data.get("signals")]
        deeper_options = [
            {
                "key": "force_dimension_detail",
                "description": "æ·±å…¥åˆ†æç‰¹å®šç»´åº¦",
                "how_to_use": f"è°ƒç”¨ get_analysis_samples_tool(song_id, focus_dimensions=['{dims_with_signals[0] if dims_with_signals else 'sentiment'}'])",
                "when_useful": "æƒ³è¯¦ç»†äº†è§£æŸä¸ªç»´åº¦çš„ä¿¡å·æ—¶",
                "available_dimensions": list(dimensions_summary.keys())
            },
            {
                "key": "force_all_samples",
                "description": "è·å–æ‰€æœ‰ç»´åº¦çš„éªŒè¯æ ·æœ¬",
                "how_to_use": "è°ƒç”¨ get_analysis_samples_tool(song_id)",
                "when_useful": "æƒ³å…¨é¢éªŒè¯æ‰€æœ‰ä¿¡å·æ—¶"
            }
        ]

        # å¦‚æœæœ‰éœ€è¦éªŒè¯çš„ä¿¡å·ï¼Œæ¨èæ·±å…¥
        if needs_verification:
            deeper_options[1]["recommended"] = True

        return {
            "status": "success",
            "layer": 1,
            "layer_name": "dimension_signals",

            "dimensions": dimensions_summary,

            "cross_dimension_signals": [
                {
                    "signal_id": sig.get("signal_id", ""),
                    "fact": sig.get("fact", ""),
                    "possible_reasons": sig.get("possible_reasons", []),
                    "ai_action": sig.get("ai_action", "")
                }
                for sig in cross_signals
            ],

            "signals_summary": {
                "total": total_signals,
                "from_dimensions": len(all_signals),
                "cross_dimension": len(cross_signals),
                "needs_verification": needs_verification
            },

            "ai_guidance": ai_guidance,

            # v0.8.4: å¼ºåˆ¶ AI æŠ¥å‘Šæ•°æ®æ¥æº
            "ai_output_requirements": ai_output_requirements,

            # v0.8.5: ç”¨æˆ·å¼ºåˆ¶æ·±å…¥é€‰é¡¹
            "deeper_options": deeper_options,

            # v0.8.6: é‡‡æ ·å»ºè®®ï¼ˆåŸºäºå„ç»´åº¦æ•°æ®å……è¶³æ€§ï¼‰
            "sampling_recommendations": sampling_recommendations if insufficient_dimensions else None
        }

    except Exception as e:
        logger.error(f"Layer 1 åˆ†æå¤±è´¥: {e}", exc_info=True)
        return {
            "status": "error",
            "error_type": "layer1_failed",
            "message": str(e),
            "song_id": song_id
        }

    finally:
        session.close()


# ============================================================
# Layer 2: éªŒè¯æ ·æœ¬
# ============================================================

def get_analysis_samples(
    song_id: str,
    focus_dimensions: List[str] = None
) -> Dict[str, Any]:
    """
    Layer 2: éªŒè¯æ ·æœ¬ - AI ç¬¬ä¸‰çœ¼çœ‹è¿™é‡Œ

    è¿”å›é”šç‚¹æ ·æœ¬å’Œå¯¹æ¯”æ ·æœ¬ï¼Œå¸®åŠ© AIï¼š
    - éªŒè¯ Layer 1 å‘ç°çš„ä¿¡å·
    - åˆ¤æ–­ç®—æ³•æ˜¯å¦è¯¯åˆ¤
    - ç†è§£è¯„è®ºåŒºçœŸå®æ°›å›´

    Args:
        song_id: æ­Œæ›²ID
        focus_dimensions: é‡ç‚¹å…³æ³¨çš„ç»´åº¦ï¼ˆå¯é€‰ï¼‰

    Returns:
        {
            "status": "success",
            "layer": 2,
            "anchors": {
                "most_liked": [...],
                "earliest": [...],
                "latest": [...],
                "longest": [...]
            },
            "contrast": {
                "high_likes_low_score": [...],
                "low_likes_but_long": [...]
            },
            "verification_tasks": [...],
            "ai_guidance": {...}
        }
    """
    session = get_session()

    try:
        # 1. è·å–æ­Œæ›²
        song = session.query(Song).filter_by(id=song_id).first()
        if not song:
            return workflow_error("song_not_found", "get_analysis_samples")

        # 2. è·å–è¯„è®º
        comments = session.query(Comment).filter_by(song_id=song_id).limit(MAX_ANALYSIS_SIZE).all()
        if not comments:
            return workflow_error("no_comments", "get_analysis_samples")

        # 3. åˆ†æç»´åº¦ä»¥è·å–æ ·æœ¬
        from mcp_server.tools.dimension_analyzers_v2 import analyze_all_dimensions_v2
        dimensions_result = analyze_all_dimensions_v2(comments)

        # 4. æå–é”šç‚¹å’Œå¯¹æ¯”æ ·æœ¬
        anchor_contrast = dimensions_result.get("anchor_contrast_samples", {})

        anchors_raw = anchor_contrast.get("anchors", {})
        contrast_raw = anchor_contrast.get("contrast", {})

        # 5. æ ¼å¼åŒ–æ ·æœ¬ï¼ˆåªä¿ç•™å…³é”®ä¿¡æ¯ï¼‰
        def format_sample(s):
            """æ ¼å¼åŒ–å•ä¸ªæ ·æœ¬"""
            if isinstance(s, str):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œè¿”å›ç®€å•ç»“æ„
                return {"content": s[:200], "likes": 0, "date": "", "algorithm_score": None}
            if isinstance(s, dict):
                return {
                    "content": s.get("content", "")[:200],
                    "likes": s.get("likes", 0),
                    "date": s.get("date", ""),
                    "algorithm_score": s.get("algorithm_score", s.get("score", None))
                }
            return {"content": str(s)[:200], "likes": 0, "date": "", "algorithm_score": None}

        # anchors çš„ç»“æ„ï¼š{purpose, most_liked, earliest, latest, longest, note}
        # åªæå–æ ·æœ¬åˆ—è¡¨å­—æ®µ
        anchor_keys = ["most_liked", "earliest", "latest", "longest"]
        formatted_anchors = {}
        for key in anchor_keys:
            samples = anchors_raw.get(key, [])
            if samples and isinstance(samples, list):
                formatted_anchors[key] = [format_sample(s) for s in samples[:5]]

        # contrast çš„ç»“æ„ï¼š{purpose, high_likes_low_score, low_likes_but_long, note}
        contrast_keys = ["high_likes_low_score", "low_likes_but_long"]
        formatted_contrast = {}
        for key in contrast_keys:
            samples = contrast_raw.get(key, [])
            if samples and isinstance(samples, list):
                formatted_contrast[key] = [format_sample(s) for s in samples[:5]]

        # 6. æ„å»ºéªŒè¯ä»»åŠ¡
        verification_tasks = []

        if formatted_contrast.get("high_likes_low_score"):
            verification_tasks.append({
                "task": "éªŒè¯é«˜èµä½åˆ†æ ·æœ¬",
                "question": "è¿™äº›é«˜èµä½†ç®—æ³•ä½åˆ†çš„è¯„è®ºæ˜¯ï¼šåè®½/ç©æ¢—ï¼Ÿè¯—æ„è¡¨è¾¾ï¼Ÿè¿˜æ˜¯çœŸå®è´Ÿé¢ï¼Ÿ",
                "samples_key": "contrast.high_likes_low_score"
            })

        if formatted_anchors.get("most_liked"):
            verification_tasks.append({
                "task": "åˆ†æé«˜èµå…±é¸£",
                "question": "æœ€é«˜èµè¯„è®ºåæ˜ äº†ä»€ä¹ˆå…±é¸£ï¼Ÿä¸æ­Œæ›²ä¸»é¢˜ç›¸å…³å—ï¼Ÿ",
                "samples_key": "anchors.most_liked"
            })

        if formatted_anchors.get("earliest") and formatted_anchors.get("latest"):
            verification_tasks.append({
                "task": "å¯¹æ¯”æ—©æœŸvsæœ€æ–°",
                "question": "è¯„è®ºåŒºæ°›å›´æœ‰å˜åŒ–å—ï¼Ÿæ—©æœŸå’Œæœ€æ–°è¯„è®ºé£æ ¼æ˜¯å¦ä¸åŒï¼Ÿ",
                "samples_key": "anchors.earliest vs anchors.latest"
            })

        # 7. æ£€æŸ¥é‡‡æ ·çº§åˆ«ï¼Œå†³å®šæ˜¯å¦æç¤ºå‡çº§
        comment_count = len(comments)
        DEEP_TARGET = 1000  # deep çº§åˆ«ç›®æ ‡
        STANDARD_TARGET = 600  # standard çº§åˆ«ç›®æ ‡

        # åˆ¤æ–­å½“å‰é‡‡æ ·çº§åˆ«
        if comment_count >= DEEP_TARGET:
            current_level = "deep"
            can_upgrade = False
        elif comment_count >= STANDARD_TARGET:
            current_level = "standard"
            can_upgrade = True
        elif comment_count >= 200:
            current_level = "quick"
            can_upgrade = True
        else:
            current_level = "minimal"
            can_upgrade = True

        # v0.8.7: æ„å»ºé‡‡æ ·å‡çº§æç¤º
        sampling_upgrade_prompt = None
        if can_upgrade:
            sampling_upgrade_prompt = {
                "should_ask_user": True,
                "current_level": current_level,
                "current_count": comment_count,
                "upgrade_to": "deep" if current_level != "standard" else "deep",
                "upgrade_target": DEEP_TARGET,
                "prompt_template": f"ğŸ“Š å½“å‰åˆ†æåŸºäº {comment_count} æ¡è¯„è®ºï¼ˆ{current_level} çº§åˆ«ï¼‰ã€‚å¦‚éœ€æ›´ç²¾ç¡®çš„åˆ†æï¼Œå¯ä»¥å‡çº§åˆ° deep çº§åˆ«ï¼ˆ{DEEP_TARGET} æ¡ï¼‰ã€‚æ˜¯å¦éœ€è¦æ›´æ·±å…¥çš„é‡‡æ ·åˆ†æï¼Ÿ",
                "action_if_yes": f"è°ƒç”¨ sample_comments_tool(song_id='{song_id}', level='deep')",
                "ai_instruction": "âš ï¸ åˆ†æå®Œæˆåï¼Œå¿…é¡»è¯¢é—®ç”¨æˆ·æ˜¯å¦éœ€è¦æ›´æ·±å…¥çš„é‡‡æ ·åˆ†æï¼"
            }

        # 8. æ„å»º AI å¼•å¯¼ - v0.8.7 å¢å¼ºç‰ˆï¼šå‘æ•£æ€è€ƒæ¡†æ¶
        ai_guidance = {
            "current_task": "é˜…è¯»æ ·æœ¬ï¼ŒéªŒè¯ Layer 1 çš„ä¿¡å·",
            "if_need_more": "è°ƒç”¨ get_raw_comments_v2_tool(song_id, year=X, min_likes=Y) è·å–æ›´å¤šåŸå§‹è¯„è®º",
            "final_output": "åŸºäºæ ·æœ¬è¯æ®ï¼Œç»™å‡ºå¯¹è¯„è®ºåŒºçš„æ•´ä½“åˆ¤æ–­",

            # v0.8.7: å‘æ•£æ€è€ƒå¼•å¯¼æ¡†æ¶
            "divergent_thinking": {
                "purpose": "ä¸è¦åªéªŒè¯ä¿¡å·ï¼Œè¦å‘ç°ä¿¡å·èƒŒåçš„æ•…äº‹",

                "cross_dimension_questions": [
                    "é«˜èµè¯„è®ºçš„å†…å®¹ç±»å‹æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆé‡‘å¥/æ•…äº‹/ç©æ¢—/ä¸“ä¸šè¯„è®ºï¼‰",
                    "é«˜èµä½åˆ†æ ·æœ¬æ­ç¤ºäº†ä»€ä¹ˆï¼Ÿï¼ˆç®—æ³•ç›²åŒº=ç”¨æˆ·çœŸæ­£è®¤å¯ä»€ä¹ˆï¼‰",
                    "æ—¶é—´çº¿ä¸Šæœ‰ä»€ä¹ˆæ¼”åŒ–ï¼Ÿï¼ˆæ—©æœŸvså¤å…´æœŸvså½“ä¸‹ï¼Œæ°›å›´å˜åŒ–ï¼‰",
                    "ç¤¾äº¤é›†ä¸­åº¦åæ˜ äº†ä»€ä¹ˆï¼Ÿï¼ˆæ˜¯ç²¾è‹±æ§åœºè¿˜æ˜¯å¤§ä¼—ç‹‚æ¬¢ï¼‰"
                ],

                "cultural_lens": [
                    "æœ‰æ²¡æœ‰æ–‡åŒ–ç°è±¡ï¼Ÿï¼ˆè°éŸ³æ¢—ã€ç©æ¢—ä¼ æ’­ã€çº¯çˆ±æ–‡åŒ–ã€æ€€æ—§æƒ…ç»ªï¼‰",
                    "è¯„è®ºåŒºçš„æœ¬è´¨æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆéŸ³ä¹è®¨è®º/æƒ…æ„Ÿæ ‘æ´/æ–‡æ¡ˆåšç‰©é¦†/ç¤¾äº¤å¹¿åœºï¼‰",
                    "å­˜åœ¨ä»€ä¹ˆ'éšæ€§è§„åˆ™'ï¼Ÿï¼ˆæŠ¢çƒ­è¯„ã€å¤åˆ¶é‡‘å¥ã€è®²æ•…äº‹æ±‚èµï¼‰"
                ],

                "algorithm_blindspots": [
                    "ç®—æ³•æŠŠä»€ä¹ˆè¯¯åˆ¤ä¸ºè´Ÿé¢ï¼Ÿï¼ˆæ„Ÿä¼¤å¼é‡‘å¥ã€åè®½ã€è¯—æ„è¡¨è¾¾ï¼‰",
                    "ç”¨æˆ·çœŸæ­£è®¤å¯ä»€ä¹ˆå†…å®¹ï¼Ÿï¼ˆé«˜èµä½åˆ†æ ·æœ¬æ˜¯æœ€å¥½çš„è¯æ®ï¼‰",
                    "ç®—æ³•ä¸ç†è§£ä»€ä¹ˆï¼Ÿï¼ˆå¦‚'ç—›è‹¦çš„ç¾å­¦ä»·å€¼'ï¼‰"
                ],

                "synthesis_prompts": [
                    "ç”¨ä¸€å¥è¯æ¦‚æ‹¬è¿™ä¸ªè¯„è®ºåŒºçš„æœ¬è´¨",
                    "è¿™ä¸ªè¯„è®ºåŒºå’Œå…¶ä»–éŸ³ä¹è¯„è®ºåŒºæœ‰ä»€ä¹ˆä¸åŒï¼Ÿ",
                    "å¦‚æœè¦ç»™åˆ«äººæ¨èçœ‹è¿™é¦–æ­Œçš„è¯„è®ºåŒºï¼Œä½ ä¼šè¯´ä»€ä¹ˆï¼Ÿ"
                ]
            }
        }

        # ç»Ÿè®¡æ ·æœ¬æ•°é‡
        anchor_count = sum(len(v) for v in formatted_anchors.values())
        contrast_count = sum(len(v) for v in formatted_contrast.values())

        # v0.8.4: AI è¾“å‡ºè¦æ±‚ - å¼ºåˆ¶ç™½ç›’åŒ– + é¿å… confirmation bias
        ai_output_requirements = {
            "must_report": [
                f"æ ·æœ¬æ¥æºï¼šé”šç‚¹æ ·æœ¬ {anchor_count} æ¡ï¼Œå¯¹æ¯”æ ·æœ¬ {contrast_count} æ¡",
                "æ¯ä¸ªåˆ¤æ–­å¿…é¡»å¼•ç”¨å…·ä½“æ ·æœ¬å†…å®¹"
            ],
            "format": "æŠ¥å‘Šç»“è®ºæ—¶å¿…é¡»å¼•ç”¨åŸæ–‡ï¼ˆå¦‚ï¼š'æ ¹æ®æ ·æœ¬ XXX...'ï¼‰",
            "avoid_confirmation_bias": [
                "ä¸è¦é¢„è®¾ç»“è®ºå†æ‰¾è¯æ®",
                "å¦‚æœæ ·æœ¬è¯æ®ä¸é¢„æœŸä¸ç¬¦ï¼Œåº”è¯¥è°ƒæ•´åˆ¤æ–­",
                "å¤šç§å¯èƒ½æ€§å¹¶å­˜æ—¶ï¼Œåº”åˆ—å‡ºæ‰€æœ‰å¯èƒ½è€Œéåªé€‰ä¸€ä¸ª"
            ],
            "objectivity": "è®©ç”¨æˆ·çœ‹åˆ°ä½ çš„æ¨ç†è¿‡ç¨‹ï¼Œè€Œä¸åªæ˜¯ç»“è®º",
            # v0.8.5: å†³ç­–é€æ˜åŒ–
            "decision_transparency": {
                "must_explain": "AI å¿…é¡»åœ¨è¾“å‡ºä¸­è¯´æ˜ï¼šæ ·æœ¬æ˜¯å¦è¶³å¤Ÿæ”¯æ’‘ç»“è®ºï¼Ÿæ˜¯å¦éœ€è¦ Layer 3ï¼ˆåŸå§‹è¯„è®ºï¼‰ï¼Ÿ",
                "example_continue": "æ ·æœ¬ä¸­å‘ç°å¼‚å¸¸æ¨¡å¼ï¼Œéœ€è¦æ›´å¤šåŸå§‹è¯„è®ºéªŒè¯ï¼Œè°ƒç”¨ get_raw_comments_v2",
                "example_stop": f"é”šç‚¹æ ·æœ¬ {anchor_count} æ¡ + å¯¹æ¯”æ ·æœ¬ {contrast_count} æ¡è¶³å¤ŸéªŒè¯ä¿¡å·ï¼Œè¾“å‡ºæœ€ç»ˆæŠ¥å‘Š"
            },

            # v0.8.7: æŠ¥å‘Šæ¨¡æ¿ - è®©æŠ¥å‘Šæ›´æ·±å…¥ã€æ›´å¥½çœ‹
            "report_template": {
                "structure": [
                    "## ğŸ“Š æ•°æ®åŸºç¡€",
                    "  - åˆ†ææ ·æœ¬ï¼šXæ¡ï¼ˆæ ·æœ¬æ„æˆï¼šçƒ­è¯„+æœ€æ–°+å¹´åº¦é‡‡æ ·ï¼‰",
                    "  - è¦†ç›–ç‡ï¼šX%ï¼ˆAPIæ€»é‡ vs é‡‡æ ·é‡ï¼‰",
                    "  - æ—¶é—´è·¨åº¦ï¼šYYYY-YYYYï¼ˆXå¹´ï¼‰",
                    "  - âš ï¸ æ•°æ®å±€é™æ€§ï¼šè¦†ç›–ç‡<1%æ—¶å¿…é¡»è¯´æ˜",
                    "",
                    "## ğŸ¯ æ ¸å¿ƒå‘ç°ï¼ˆ3-5ä¸ªï¼‰",
                    "  - æ¯ä¸ªå‘ç°å¿…é¡»ï¼šæœ‰æ ‡é¢˜ + æœ‰æ ·æœ¬è¯æ® + æœ‰è§£è¯»",
                    "  - å¼•ç”¨æ ¼å¼ï¼š\"å…·ä½“è¯„è®ºå†…å®¹\"ï¼ˆXä¸‡èµï¼Œæ—¥æœŸï¼‰",
                    "",
                    "## ğŸ§  æ·±å±‚æœºåˆ¶",
                    "  - ä¸ºä»€ä¹ˆæ˜¯è¿™é¦–æ­Œï¼Ÿï¼ˆæ­Œæ›²ç‰¹è´¨å¦‚ä½•å‚¬ç”Ÿè¯„è®ºåŒºæ–‡åŒ–ï¼‰",
                    "  - è¯„è®ºåŒºæ¼”åŒ–è·¯å¾„ï¼ˆæ—©æœŸâ†’ä¸­æœŸâ†’ç°åœ¨ï¼‰",
                    "",
                    "## ğŸ“Œ ä¸å…¶ä»–è¯„è®ºåŒºå¯¹æ¯”ï¼ˆè¡¨æ ¼ï¼‰",
                    "  | ç»´åº¦ | æœ¬è¯„è®ºåŒº | å…¸å‹è¯„è®ºåŒº |",
                    "",
                    "## ğŸ” ä¸€å¥è¯æ€»ç»“",
                    "  - ç”¨ä¸€å¥è¯æ¦‚æ‹¬è¯„è®ºåŒºæœ¬è´¨",
                    "",
                    "## ğŸ’¡ æ¨èè¯­",
                    "  - å¦‚æœè¦ç»™åˆ«äººæ¨èçœ‹è¿™ä¸ªè¯„è®ºåŒºï¼Œä½ ä¼šè¯´ä»€ä¹ˆï¼Ÿ"
                ],
                "formatting_rules": [
                    "ä½¿ç”¨ emoji ä½œä¸ºç« èŠ‚æ ‡é¢˜å‰ç¼€",
                    "é«˜èµè¯„è®ºç”¨å¼•ç”¨æ ¼å¼ï¼ˆ>ï¼‰çªå‡ºæ˜¾ç¤º",
                    "æ•°æ®å¯¹æ¯”ç”¨è¡¨æ ¼å‘ˆç°",
                    "å…³é”®å‘ç°ç”¨**åŠ ç²—**å¼ºè°ƒ",
                    "æ¯ä¸ªåˆ¤æ–­å¿…é¡»é™„å¸¦æ ·æœ¬è¯æ®"
                ],
                "depth_requirements": [
                    "ä¸è¦åªæè¿°ç°è±¡ï¼Œè¦è§£é‡ŠåŸå› ",
                    "ä¸è¦åªåˆ—ä¸¾æ•°æ®ï¼Œè¦æŒ–æ˜æ´å¯Ÿ",
                    "ä¸è¦åªéªŒè¯ä¿¡å·ï¼Œè¦å‘ç°ä¿¡å·èƒŒåçš„æ•…äº‹",
                    "è¦æœ‰'è¿™ä¸ªè¯„è®ºåŒºç‹¬ç‰¹åœ¨å“ªé‡Œ'çš„è§†è§’"
                ]
            }
        }

        # v0.8.5: ç”¨æˆ·å¼ºåˆ¶æ·±å…¥é€‰é¡¹
        deeper_options = [
            {
                "key": "force_more_samples",
                "description": "è·å–æ›´å¤šåŸå§‹è¯„è®ºï¼ˆå½“å‰æ ·æœ¬ä¸è¶³æ—¶ï¼‰",
                "how_to_use": "è°ƒç”¨ get_raw_comments_v2_tool(song_id, limit=50)",
                "when_useful": "å½“æ ·æœ¬æ•°é‡ä¸è¶³ä»¥å¾—å‡ºå¯é ç»“è®ºæ—¶"
            },
            {
                "key": "force_high_likes_only",
                "description": "åªçœ‹é«˜èµè¯„è®ºï¼ˆ>=1000èµï¼‰",
                "how_to_use": "è°ƒç”¨ get_raw_comments_v2_tool(song_id, min_likes=1000)",
                "when_useful": "æƒ³äº†è§£ç¤¾åŒºè®¤å¯çš„ä¸»æµè§‚ç‚¹"
            },
            {
                "key": "force_specific_year",
                "description": "æŸ¥çœ‹ç‰¹å®šå¹´ä»½çš„è¯„è®º",
                "how_to_use": "è°ƒç”¨ get_raw_comments_v2_tool(song_id, year=XXXX)",
                "when_useful": "æƒ³æ·±å…¥åˆ†ææŸä¸ªæ—¶æœŸçš„è¯„è®ºåŒºæ°›å›´"
            }
        ]

        # æ ¹æ®æ ·æœ¬æƒ…å†µè°ƒæ•´å»ºè®®
        total_samples = anchor_count + contrast_count
        if total_samples < 10:
            deeper_options[0]["recommended"] = True

        return {
            "status": "success",
            "layer": 2,
            "layer_name": "verification_samples",

            "anchors": formatted_anchors,
            "contrast": formatted_contrast,

            "sample_counts": {
                "anchors": anchor_count,
                "contrast": contrast_count,
                "total": total_samples
            },

            "verification_tasks": verification_tasks,

            "ai_guidance": ai_guidance,

            # v0.8.4: å¼ºåˆ¶ AI æŠ¥å‘Šæ•°æ®æ¥æº + é¿å… confirmation bias
            "ai_output_requirements": ai_output_requirements,

            # v0.8.5: ç”¨æˆ·å¼ºåˆ¶æ·±å…¥é€‰é¡¹
            "deeper_options": deeper_options,

            # v0.8.7: é‡‡æ ·å‡çº§æç¤ºï¼ˆå¦‚æœä¸æ˜¯ deep çº§åˆ«ï¼‰
            "sampling_upgrade_prompt": sampling_upgrade_prompt
        }

    except Exception as e:
        logger.error(f"Layer 2 åˆ†æå¤±è´¥: {e}", exc_info=True)
        return {
            "status": "error",
            "error_type": "layer2_failed",
            "message": str(e),
            "song_id": song_id
        }

    finally:
        session.close()


# ============================================================
# Layer 3: åŸå§‹è¯„è®º (å·²æœ‰ get_raw_comments_v2)
# ============================================================

# ä» comprehensive_analysis_v2.py å¯¼å…¥
from mcp_server.tools.comprehensive_analysis_v2 import get_raw_comments_v2


# ============================================================
# å¯¼å‡º
# ============================================================

__all__ = [
    "get_analysis_overview",     # Layer 0
    "get_analysis_signals",      # Layer 1
    "get_analysis_samples",      # Layer 2
    "get_raw_comments_v2",       # Layer 3
]
