"""
æ—¶é—´çº¿åˆ†æå·¥å…·æ¨¡å— - Feature 1: è¯„è®ºæƒ…æ„Ÿæ—¶é—´çº¿
åˆ†æè¯„è®ºæƒ…æ„Ÿéšæ—¶é—´çš„å˜åŒ–è¶‹åŠ¿ï¼Œå‘ç°"ç½‘æŠ‘äº‘"ç°è±¡ç­‰è½¬æŠ˜ç‚¹

Author: 1mht + Claude
Version: v0.7.0
Date: 2025-12-30
"""

import sys
import os
from datetime import datetime
from typing import Dict, Any, List, Optional
from collections import defaultdict

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
netease_path = os.path.join(project_root, 'netease_cloud_music')
if netease_path not in sys.path:
    sys.path.insert(0, netease_path)

from database import init_db, Song, Comment
from .workflow_errors import workflow_error
from .sentiment_analysis import get_analyzer, MAX_ANALYSIS_SIZE

# ===== å¸¸é‡é…ç½® =====
MIN_COMMENTS_PER_PERIOD = 5     # æ¯ä¸ªæ—¶é—´æ®µæœ€å°‘è¯„è®ºæ•°
DEFAULT_SAMPLE_PER_PERIOD = 50  # é»˜è®¤æ¯æ—¶é—´æ®µé‡‡æ ·æ•°
MAX_SAMPLE_PER_PERIOD = 200     # æ¯æ—¶é—´æ®µæœ€å¤§é‡‡æ ·æ•°

# ===== v0.7.1: Workflow å¼ºåˆ¶æ ¡éªŒé˜ˆå€¼ =====
TIMELINE_MIN_COMMENTS = 100      # æ—¶é—´çº¿åˆ†ææœ€ä½è¯„è®ºæ•°
TIMELINE_MIN_YEARS = 2           # æ—¶é—´çº¿åˆ†ææœ€ä½è¦†ç›–å¹´æ•°


def get_session():
    """è·å–æ•°æ®åº“session"""
    db_path = os.path.join(project_root, 'data', 'music_data_v2.db')
    return init_db(f'sqlite:///{db_path}')


def _timestamp_to_period(timestamp: int, granularity: str) -> str:
    """å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºæ—¶é—´æ®µæ ‡ç­¾

    Args:
        timestamp: æ¯«ç§’çº§æ—¶é—´æˆ³
        granularity: "year" / "quarter" / "month"

    Returns:
        æ—¶é—´æ®µæ ‡ç­¾ï¼Œå¦‚ "2020", "2020-Q3", "2020-07"
    """
    try:
        dt = datetime.fromtimestamp(timestamp / 1000)

        if granularity == "year":
            return str(dt.year)
        elif granularity == "quarter":
            quarter = (dt.month - 1) // 3 + 1
            return f"{dt.year}-Q{quarter}"
        elif granularity == "month":
            return f"{dt.year}-{dt.month:02d}"
        else:
            return str(dt.year)
    except:
        return "unknown"


def _calculate_sentiment_stats(comments: List[Comment], analyzer) -> Dict[str, Any]:
    """è®¡ç®—ä¸€ç»„è¯„è®ºçš„æƒ…æ„Ÿç»Ÿè®¡

    Args:
        comments: è¯„è®ºåˆ—è¡¨
        analyzer: æƒ…æ„Ÿåˆ†æå™¨å®ä¾‹

    Returns:
        {
            "sample_size": 50,
            "avg_sentiment": 0.65,
            "sentiment_distribution": {"positive": 30, "neutral": 12, "negative": 8},
            "top_keywords": ["é’æ˜¥", "æ€€å¿µ", ...]
        }
    """
    if not comments:
        return None

    scores = []
    valid_contents = []

    for c in comments:
        if not c.content or len(c.content) < 3:
            continue
        try:
            score = analyzer.analyze(c.content)
            scores.append(score)
            valid_contents.append(c.content)
        except:
            continue

    if not scores:
        return None

    # è®¡ç®—åˆ†å¸ƒ
    positive = sum(1 for s in scores if s >= 0.6)
    negative = sum(1 for s in scores if s <= 0.4)
    neutral = len(scores) - positive - negative

    avg_score = sum(scores) / len(scores)

    # ç®€å•å…³é”®è¯æå–ï¼ˆé«˜é¢‘è¯ï¼‰
    top_keywords = _extract_simple_keywords(valid_contents, top_n=5)

    return {
        "sample_size": len(scores),
        "avg_sentiment": round(avg_score, 3),
        "sentiment_distribution": {
            "positive": positive,
            "neutral": neutral,
            "negative": negative
        },
        "positive_rate": round(positive / len(scores), 3) if scores else 0,
        "negative_rate": round(negative / len(scores), 3) if scores else 0,
        "top_keywords": top_keywords
    }


def _extract_simple_keywords(texts: List[str], top_n: int = 5) -> List[str]:
    """ç®€å•å…³é”®è¯æå–ï¼ˆåŸºäºjiebaåˆ†è¯ + è¯é¢‘ï¼‰

    Args:
        texts: æ–‡æœ¬åˆ—è¡¨
        top_n: è¿”å›å‰Nä¸ªå…³é”®è¯

    Returns:
        å…³é”®è¯åˆ—è¡¨
    """
    try:
        import jieba
    except ImportError:
        return []

    # åœç”¨è¯
    stopwords = {
        'çš„', 'äº†', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'ä»¬', 'è¿™', 'é‚£', 'æœ‰', 'åœ¨', 'å’Œ', 'ä¸',
        'å°±', 'éƒ½', 'ä¹Ÿ', 'åˆ', 'è¢«', 'æŠŠ', 'ç»™', 'è®©', 'å‘', 'ä»', 'åˆ°', 'ä¸º', 'å¯¹', 'ç€',
        'å¾ˆ', 'å¤ª', 'å¥½', 'çœŸ', 'å•Š', 'å§', 'å‘¢', 'å“¦', 'å—¯', 'å“ˆ', 'å‘€', 'å“‡', 'å“', 'å”‰',
        'ä¸€ä¸ª', 'ä¸€ç§', 'ä¸€ä¸‹', 'ä¸€äº›', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'è¿™ä¸ª', 'é‚£ä¸ª', 'æ²¡æœ‰', 'ä¸æ˜¯',
        'å¯ä»¥', 'å› ä¸º', 'æ‰€ä»¥', 'å¦‚æœ', 'ä½†æ˜¯', 'è™½ç„¶', 'è¿˜æ˜¯', 'æˆ–è€…', 'è€Œä¸”', 'ç„¶å',
        'æ­Œ', 'æ­Œæ›²', 'éŸ³ä¹', 'è¯„è®º', 'å¬', 'å”±', 'é¦–'  # éŸ³ä¹ç›¸å…³é€šç”¨è¯
    }

    word_count = defaultdict(int)

    for text in texts:
        words = jieba.cut(text)
        for word in words:
            word = word.strip()
            if len(word) >= 2 and word not in stopwords:
                word_count[word] += 1

    # æ’åºå¹¶è¿”å›top_n
    sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)
    return [word for word, count in sorted_words[:top_n]]


def _detect_turning_points(timeline: List[Dict]) -> List[Dict]:
    """æ£€æµ‹æƒ…æ„Ÿè½¬æŠ˜ç‚¹

    Args:
        timeline: æ—¶é—´çº¿æ•°æ®

    Returns:
        è½¬æŠ˜ç‚¹åˆ—è¡¨
    """
    turning_points = []

    if len(timeline) < 2:
        return turning_points

    for i in range(1, len(timeline)):
        prev = timeline[i-1]
        curr = timeline[i]

        if prev.get("avg_sentiment") is None or curr.get("avg_sentiment") is None:
            continue

        change = curr["avg_sentiment"] - prev["avg_sentiment"]

        # å˜åŒ–è¶…è¿‡0.1è§†ä¸ºè½¬æŠ˜ç‚¹
        if abs(change) >= 0.1:
            direction = "ä¸‹é™" if change < 0 else "ä¸Šå‡"

            # å°è¯•æ¨æ–­åŸå› 
            possible_reason = None
            if change < -0.15 and "2020" in curr["period"]:
                possible_reason = "å¯èƒ½ä¸'ç½‘æŠ‘äº‘'æ–‡åŒ–å…´èµ·ç›¸å…³"
            elif change > 0.15:
                possible_reason = "è¯„è®ºæ°›å›´å¥½è½¬"

            turning_points.append({
                "period": curr["period"],
                "change": round(change, 3),
                "direction": direction,
                "from_score": prev["avg_sentiment"],
                "to_score": curr["avg_sentiment"],
                "possible_reason": possible_reason
            })

    return turning_points


def _determine_trend(timeline: List[Dict]) -> str:
    """åˆ¤æ–­æ•´ä½“è¶‹åŠ¿

    Args:
        timeline: æ—¶é—´çº¿æ•°æ®

    Returns:
        "rising" / "stable" / "declining"
    """
    if len(timeline) < 2:
        return "unknown"

    # å–é¦–å°¾æœ‰æ•ˆæ•°æ®
    first_valid = None
    last_valid = None

    for t in timeline:
        if t.get("avg_sentiment") is not None:
            if first_valid is None:
                first_valid = t["avg_sentiment"]
            last_valid = t["avg_sentiment"]

    if first_valid is None or last_valid is None:
        return "unknown"

    diff = last_valid - first_valid

    if diff > 0.1:
        return "rising"
    elif diff < -0.1:
        return "declining"
    else:
        return "stable"


def analyze_sentiment_timeline(
    song_id: str,
    time_granularity: str = "year",
    sample_per_period: int = DEFAULT_SAMPLE_PER_PERIOD
) -> Dict[str, Any]:
    """åˆ†æè¯„è®ºæƒ…æ„Ÿéšæ—¶é—´çš„å˜åŒ–è¶‹åŠ¿

    æ ¸å¿ƒåŠŸèƒ½ï¼šå‘ç°æƒ…æ„Ÿè½¬æŠ˜ç‚¹ï¼Œå¦‚"ç½‘æŠ‘äº‘"ç°è±¡ä½•æ—¶å¼€å§‹

    âœ… v0.7.1: æ”¯æŒå†…éƒ¨è‡ªåŠ¨é‡‡æ ·ï¼

    ğŸ“‹ ç®€åŒ–çš„è°ƒç”¨æ–¹å¼:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ç›´æ¥è°ƒç”¨: analyze_sentiment_timeline_tool(song_id)          â”‚
    â”‚                                                             â”‚
    â”‚ å·¥å…·å†…éƒ¨è‡ªåŠ¨å¤„ç†:                                           â”‚
    â”‚ - æ£€æŸ¥æ•°æ®åº“è¯„è®ºæ•°é‡å’Œæ—¶é—´è¦†ç›–                               â”‚
    â”‚ - å¦‚æœè¯„è®º < 100æ¡ æˆ– è¦†ç›– < 2å¹´ â†’ è‡ªåŠ¨åˆ†å±‚é‡‡æ ·             â”‚
    â”‚ - é‡‡æ ·ç­–ç•¥(v2.2): çƒ­è¯„15æ¡ + æœ€æ–°50æ¡ + å†å²11å¹´(æ¯å¹´50æ¡)  â”‚
    â”‚ - è¿”å›ç»“æœä¸­åŒ…å« sampling_info å­—æ®µè¯´æ˜é‡‡æ ·è¯¦æƒ…             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    âœ… æ­£ç¡®ç”¨æ³•: ç›´æ¥è°ƒç”¨æ­¤å·¥å…·å³å¯
    â„¹ï¸ å¯é€‰æ­¥éª¤: å…ˆè°ƒç”¨ get_comments_metadata_tool äº†è§£æ•°æ®çŠ¶æ€

    ğŸ“Š é‡‡æ ·ç­–ç•¥ (v2.2 for timeline):
    - Layer 1: çƒ­è¯„15æ¡ (APIå›ºå®šè¿”å›)
    - Layer 2: æœ€æ–°50æ¡ (offsetç¿»é¡µ)
    - Layer 3: å†å²åˆ†å±‚ (cursoræŒ‰å¹´è·³è½¬ï¼Œæ¯å¹´50æ¡ï¼Œå…±11å¹´)
    - æ€»è®¡: çº¦600æ¡ï¼Œè¦†ç›–æ­Œæ›²å‘å¸ƒä»¥æ¥çš„å®Œæ•´æ—¶é—´çº¿

    Args:
        song_id: æ­Œæ›²IDï¼ˆéœ€å…ˆè°ƒç”¨confirm_song_selectionè·å–ï¼‰
        time_granularity: æ—¶é—´ç²’åº¦
            - "year": æŒ‰å¹´èšåˆï¼ˆé€‚åˆé•¿å‘¨æœŸæ­Œæ›²ï¼Œå¦‚ã€Šæ™´å¤©ã€‹å‘å¸ƒ12å¹´ï¼‰
            - "quarter": æŒ‰å­£åº¦ï¼ˆé€‚åˆ2-3å¹´å†…çš„æ­Œæ›²ï¼‰
            - "month": æŒ‰æœˆï¼ˆé€‚åˆ1å¹´å†…çš„æ–°æ­Œï¼‰
        sample_per_period: æ¯ä¸ªæ—¶é—´æ®µé‡‡æ ·è¯„è®ºæ•°
            - é»˜è®¤50ï¼Œå»ºè®®30-100
            - å¤ªå°‘ï¼šç»“æœä¸ç¨³å®š
            - å¤ªå¤šï¼šå¤„ç†æ…¢

    Returns:
        {
            "status": "success",
            "song_info": {"id": "185811", "name": "æ™´å¤©", "artist": "å‘¨æ°ä¼¦"},
            "time_range": {"start": "2013-07-15", "end": "2025-12-30", "span_years": 12.5},
            "granularity": "year",

            "timeline": [
                {
                    "period": "2013",
                    "sample_size": 50,
                    "avg_sentiment": 0.72,
                    "sentiment_distribution": {"positive": 35, "neutral": 10, "negative": 5},
                    "positive_rate": 0.70,
                    "negative_rate": 0.10,
                    "top_keywords": ["å¥½å¬", "ç»å…¸", "å‘¨æ°ä¼¦"]
                },
                ...
            ],

            "insights": {
                "trend": "declining",
                "overall_change": -0.27,
                "turning_points": [
                    {"period": "2020", "change": -0.15, "possible_reason": "ç½‘æŠ‘äº‘æ–‡åŒ–"}
                ],
                "summary": "æƒ…æ„Ÿä»2013å¹´çš„0.72ä¸‹é™åˆ°2025å¹´çš„0.45"
            },

            "data_quality": {
                "total_comments_used": 500,
                "periods_with_data": 10,
                "avg_sample_per_period": 50,
                "confidence": "high"
            },

            "suggestion": "å‘ç°2020å¹´æƒ…æ„Ÿæ˜æ˜¾ä¸‹é™ï¼Œå¯èƒ½ä¸'ç½‘æŠ‘äº‘'ç°è±¡ç›¸å…³",
            "next_step": "å¦‚éœ€æ·±å…¥åˆ†ææŸæ—¶é—´æ®µï¼Œå¯è°ƒç”¨get_comments_by_pagesæŒ‡å®šæ—¶é—´èŒƒå›´"
        }

    ç¤ºä¾‹å¯¹è¯:
        ç”¨æˆ·: "åˆ†æè¿™é¦–æ­Œçš„æƒ…æ„Ÿå˜åŒ–"
        AI: [è°ƒç”¨ analyze_sentiment_timeline(song_id="185811")]
            å‘ç°ã€Šæ™´å¤©ã€‹çš„è¯„è®ºæƒ…æ„Ÿä»2013å¹´çš„0.72ä¸‹é™åˆ°2020å¹´çš„0.45ï¼Œ
            è½¬æŠ˜ç‚¹åœ¨2020å¹´ï¼Œä¸"ç½‘æŠ‘äº‘"æ–‡åŒ–å…´èµ·æ—¶é—´å»åˆã€‚
    """
    session = get_session()

    try:
        # ===== 1. å‚æ•°éªŒè¯ =====
        if time_granularity not in ["year", "quarter", "month"]:
            return {
                "status": "error",
                "error_type": "invalid_parameter",
                "message": f"æ— æ•ˆçš„æ—¶é—´ç²’åº¦: {time_granularity}",
                "valid_options": ["year", "quarter", "month"],
                "suggestion": "yearé€‚åˆè€æ­Œï¼Œmonthé€‚åˆæ–°æ­Œ"
            }

        sample_per_period = min(max(sample_per_period, 10), MAX_SAMPLE_PER_PERIOD)

        # ===== 2. è·å–æ­Œæ›²ä¿¡æ¯ =====
        song = session.query(Song).filter_by(id=song_id).first()
        if not song:
            return workflow_error("song_not_found", "analyze_sentiment_timeline_tool")

        # ===== 3. è·å–è¯„è®º =====
        comments = session.query(Comment).filter_by(song_id=song_id).all()

        if not comments:
            return workflow_error("no_comments", "analyze_sentiment_timeline_tool")

        # è¿‡æ»¤æ— æ•ˆæ—¶é—´æˆ³
        valid_comments = [c for c in comments if getattr(c, "timestamp", None) and c.timestamp > 0]

        # ===== v0.7.1: è‡ªåŠ¨åˆ†å±‚é‡‡æ · =====
        auto_sampled = False
        sampling_stats = None

        # æ£€æŸ¥è¯„è®ºæ•°é‡æˆ–æ—¶é—´è¦†ç›–æ˜¯å¦ä¸è¶³
        need_sampling = False
        if len(valid_comments) < TIMELINE_MIN_COMMENTS:
            need_sampling = True
            print(f"[è‡ªåŠ¨é‡‡æ ·] è¯„è®ºæ•°ä¸è¶³({len(valid_comments)}<{TIMELINE_MIN_COMMENTS})ï¼Œå¯åŠ¨åˆ†å±‚é‡‡æ ·...")
        else:
            timestamps_check = [c.timestamp for c in valid_comments]
            min_ts_check, max_ts_check = min(timestamps_check), max(timestamps_check)
            span_years_check = (max_ts_check - min_ts_check) / (1000 * 60 * 60 * 24 * 365)
            if span_years_check < TIMELINE_MIN_YEARS:
                need_sampling = True
                print(f"[è‡ªåŠ¨é‡‡æ ·] æ—¶é—´è¦†ç›–ä¸è¶³({span_years_check:.1f}<{TIMELINE_MIN_YEARS}å¹´)ï¼Œå¯åŠ¨åˆ†å±‚é‡‡æ ·...")

        if need_sampling:
            try:
                from .pagination_sampling import full_stratified_sample
                sample_result = full_stratified_sample(song_id, analysis_type="timeline")

                if sample_result.get('all_comments'):
                    sampled_comments = sample_result['all_comments']
                    auto_sampled = True
                    sampling_stats = sample_result.get('stats', {})

                    # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼ï¼ˆæ¨¡æ‹ŸCommentå¯¹è±¡ï¼‰
                    class CommentLike:
                        def __init__(self, data):
                            self.content = data.get('content', '')
                            self.liked_count = data.get('liked_count', 0)
                            self.timestamp = data.get('timestamp', 0)

                    valid_comments = [CommentLike(c) for c in sampled_comments if c.get('timestamp', 0) > 0]
                    print(f"[è‡ªåŠ¨é‡‡æ ·] å®Œæˆ! è·å–{len(valid_comments)}æ¡è¯„è®ºï¼Œè¦†ç›–{sampling_stats.get('years_covered', 0)}å¹´")
                else:
                    return {
                        "status": "workflow_error",
                        "error_type": "sampling_failed",
                        "message": f"è‡ªåŠ¨é‡‡æ ·å¤±è´¥",
                        "song_id": song_id,
                        "song_name": song.name
                    }
            except Exception as e:
                print(f"[è‡ªåŠ¨é‡‡æ ·] å¼‚å¸¸: {e}")
                return {
                    "status": "workflow_error",
                    "error_type": "sampling_error",
                    "message": f"è‡ªåŠ¨é‡‡æ ·å¼‚å¸¸: {str(e)}",
                    "song_id": song_id,
                    "song_name": song.name
                }

        if len(valid_comments) < 10:
            return {
                "status": "error",
                "error_type": "insufficient_data",
                "message": f"æœ‰æ•ˆè¯„è®ºå¤ªå°‘ï¼ˆ{len(valid_comments)}æ¡ï¼‰ï¼Œæ— æ³•è¿›è¡Œæ—¶é—´çº¿åˆ†æ",
                "suggestion": "éœ€è¦æ›´å¤šå¸¦æ—¶é—´æˆ³çš„è¯„è®ºæ•°æ®",
                "next_step": f"è°ƒç”¨ crawl_all_comments_tool(song_id='{song_id}') è·å–æ›´å¤šæ•°æ®"
            }

        # æŒ‰æ—¶é—´åˆ†æ¡¶
        buckets = defaultdict(list)
        for c in valid_comments:
            period = _timestamp_to_period(c.timestamp, time_granularity)
            if period != "unknown":
                buckets[period].append(c)

        if not buckets:
            return {
                "status": "error",
                "error_type": "no_valid_periods",
                "message": "æ— æ³•æŒ‰æ—¶é—´åˆ†ç»„ï¼Œè¯„è®ºæ—¶é—´æ•°æ®å¯èƒ½æœ‰é—®é¢˜"
            }

        # ===== 4. è®¡ç®—æ—¶é—´èŒƒå›´ =====
        timestamps = [c.timestamp for c in valid_comments]
        min_ts = min(timestamps)
        max_ts = max(timestamps)

        start_date = datetime.fromtimestamp(min_ts / 1000).strftime("%Y-%m-%d")
        end_date = datetime.fromtimestamp(max_ts / 1000).strftime("%Y-%m-%d")
        span_years = round((max_ts - min_ts) / (1000 * 60 * 60 * 24 * 365), 1)

        # ===== 5. åˆå§‹åŒ–åˆ†æå™¨ =====
        analyzer = get_analyzer("simple")

        # ===== 6. åˆ†ææ¯ä¸ªæ—¶é—´æ®µ =====
        timeline = []
        total_sampled = 0

        # æŒ‰æ—¶é—´æ’åº
        sorted_periods = sorted(buckets.keys())

        for period in sorted_periods:
            period_comments = buckets[period]

            # é‡‡æ ·
            if len(period_comments) > sample_per_period:
                import random
                sampled = random.sample(period_comments, sample_per_period)
            else:
                sampled = period_comments

            total_sampled += len(sampled)

            # è®¡ç®—ç»Ÿè®¡
            stats = _calculate_sentiment_stats(sampled, analyzer)

            if stats:
                timeline.append({
                    "period": period,
                    "total_in_period": len(period_comments),
                    **stats
                })
            else:
                # æ•°æ®ä¸è¶³çš„æ—¶é—´æ®µ
                timeline.append({
                    "period": period,
                    "total_in_period": len(period_comments),
                    "sample_size": 0,
                    "avg_sentiment": None,
                    "note": "è¯¥æ—¶é—´æ®µæœ‰æ•ˆè¯„è®ºä¸è¶³"
                })

        # ===== 7. ç”Ÿæˆæ´å¯Ÿ =====
        trend = _determine_trend(timeline)
        turning_points = _detect_turning_points(timeline)

        # è®¡ç®—æ€»ä½“å˜åŒ–
        valid_timeline = [t for t in timeline if t.get("avg_sentiment") is not None]
        overall_change = 0
        summary = ""

        if len(valid_timeline) >= 2:
            first_score = valid_timeline[0]["avg_sentiment"]
            last_score = valid_timeline[-1]["avg_sentiment"]
            overall_change = round(last_score - first_score, 3)

            first_period = valid_timeline[0]["period"]
            last_period = valid_timeline[-1]["period"]
            summary = f"æƒ…æ„Ÿä»{first_period}å¹´çš„{first_score}{'ä¸‹é™' if overall_change < 0 else 'ä¸Šå‡'}åˆ°{last_period}å¹´çš„{last_score}"

        # ===== 8. æ•°æ®è´¨é‡è¯„ä¼° =====
        periods_with_data = len([t for t in timeline if t.get("sample_size", 0) > 0])
        avg_sample = total_sampled / periods_with_data if periods_with_data > 0 else 0

        if avg_sample >= 40 and periods_with_data >= 3:
            confidence = "high"
        elif avg_sample >= 20 and periods_with_data >= 2:
            confidence = "medium"
        else:
            confidence = "low"

        # ===== 9. æ„å»ºè¿”å›ç»“æœ =====
        result = {
            "status": "success",
            "song_info": {
                "id": song_id,
                "name": song.name,
                "artist": song.artists[0].name if song.artists else "Unknown"
            },
            "time_range": {
                "start": start_date,
                "end": end_date,
                "span_years": span_years
            },
            "granularity": time_granularity,
            "timeline": timeline,
            "insights": {
                "trend": trend,
                "trend_cn": {"rising": "ä¸Šå‡", "stable": "å¹³ç¨³", "declining": "ä¸‹é™"}.get(trend, "æœªçŸ¥"),
                "overall_change": overall_change,
                "turning_points": turning_points,
                "summary": summary
            },
            "data_quality": {
                "total_comments_in_db": len(comments),
                "total_comments_used": total_sampled,
                "periods_analyzed": len(timeline),
                "periods_with_data": periods_with_data,
                "avg_sample_per_period": round(avg_sample, 1),
                "confidence": confidence
            }
        }

        # ç”Ÿæˆå»ºè®®
        if turning_points:
            biggest_change = max(turning_points, key=lambda x: abs(x["change"]))
            result["suggestion"] = f"å‘ç°{biggest_change['period']}æƒ…æ„Ÿ{biggest_change['direction']}æ˜æ˜¾ï¼ˆ{biggest_change['change']:+.2f}ï¼‰"
            if biggest_change.get("possible_reason"):
                result["suggestion"] += f"ï¼Œ{biggest_change['possible_reason']}"
        else:
            result["suggestion"] = f"æƒ…æ„Ÿæ•´ä½“{result['insights']['trend_cn']}ï¼Œæ— æ˜æ˜¾è½¬æŠ˜ç‚¹"

        result["next_step"] = "å¦‚éœ€æ·±å…¥åˆ†ææŸä¸ªæ—¶é—´æ®µï¼Œå¯è°ƒç”¨get_comments_by_pagesæŒ‡å®šæ—¶é—´èŒƒå›´"

        # ===== v0.7.1: æ·»åŠ é‡‡æ ·ä¿¡æ¯ï¼ˆé€æ˜å±•ç¤ºï¼‰ =====
        if auto_sampled and sampling_stats:
            result["sampling_info"] = {
                "auto_sampled": True,
                "strategy": "stratified_v2.2",
                "hot_count": sampling_stats.get('hot_count', 0),
                "recent_count": sampling_stats.get('recent_count', 0),
                "historical_count": sampling_stats.get('historical_count', 0),
                "total_unique": sampling_stats.get('total_unique', 0),
                "years_covered": sampling_stats.get('years_covered', 0),
                "year_list": sampling_stats.get('year_list', []),
                "note": "æ•°æ®é€šè¿‡è‡ªåŠ¨åˆ†å±‚é‡‡æ ·è·å–ï¼ˆçƒ­è¯„+æœ€æ–°+å†å²cursorè·³è½¬ï¼‰"
            }

        return result

    except Exception as e:
        return {
            "status": "error",
            "error_type": "analysis_failed",
            "message": f"åˆ†æå¤±è´¥: {str(e)}",
            "song_id": song_id
        }

    finally:
        session.close()
